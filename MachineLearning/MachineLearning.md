# 一、机器学习模型

## 1、学习模型分类

### 1.1 有监督学习模型

所有回归和分类算法都属于有监督学习。

监督学习：通过已有的一部分输入数据与输出数据的对应关系，生成一个函数，将输入映射到合适的输出。

![有监督模型](F:\面试\图片\有监督模型.jpg)

### 1.2 无监督学习模型

直接对输入数据即进行建模，例如强化学习、K-Means聚类、自编码、受限波尔兹曼机。

![](F:\面试\图片\无监督模型.png)

### 1.3 概率模型

![](F:\面试\图片\概率模型.png)

|      | 定义                                               | 算法                                 | 案例         |
| ---- | -------------------------------------------------- | :----------------------------------- | ------------ |
| 分类 | 对**离散**随机变量建模预测的监督学习算法           | LR、SVM、KNN、决策树、随机森林、GBDT | 垃圾邮件分类 |
| 回归 | 对**连续**随机变量建模预测的监督学习算法           | 非线性回归、SVR、随机森林            | 房价预测     |
| 聚类 | 基于数据的内部规律，寻找其属于不同类别的无监督算法 | K-Means、GMM（高斯混合模型）         |              |

### 1.4 生成模型和判别模型

### 1.4.1 生成模型 （朴素贝叶斯、高斯混合模型）

由数据学习联合概率密度P(X, Y)，然后**求出条件概率分布P(Y|X)作为预测的模型。**

### 1.4.2 判别模型 （K近邻、SVM、神经网络、boosting、LR、线性回归）

**由数据直接学习决策函数Y=f(X)或条件概率P(Y|X)**作为预测的模型。

### 1.4.3 优缺点

- 生成方法：可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度

- 判别方法：不能反映训练数据本身的特性。但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异

## 2、线性回归

目的：用线性函数去拟合数据

### 2.1 什么是回归分析

回归分析是一种预测性的建模技术，它研究的是因变量和自变量之间的关系。这种技术通常用于预测分析，时间序列模型以及发现变量之间的因果关系。通常使用曲线来拟合数据点，目标是使得曲线到数据点的差异最小。

### 2.2 线性回归

线性回归是回归问题中的一种，线性回归假设目标值与特征之间线性相关，即满足一个多元一次方程。通过构建损失函数，来求解损失函数最小时的参数w和b。
$$
y_{pre}=wx+b
$$
ypre为预测值，自变量x和因变量y是已知的，而我们想实现的是预测新增一个x，其对应的y是多少。因此，为了构建这个函数关系，目标是通过已知数据点，求解线性模型中w和b两个参数。

### 2.3 损失函数

针对任何模型求解问题，最终都可以得到一组预测值ypre，对比已有的真实值y，可以将损失函数定义如下：
$$
L=\frac{1}{n}\sum_{i=1}^{n}({y_{i,pre}-y_i})^2
$$

$$
L(w,b)=\frac{1}{n}\sum_{i=1}^{n}({wx_i+b-y_i})^2
$$

即预测值和真实值的平均平方距离，称之为均方误差MSE。目标是求解最小化L时w和b的值。

### 2.4 求解方式

- 最小二乘法
  $$
  \frac{\partial{L}}{\partial{w}}=2(w\sum_{i=1}^{n}x^2-\sum_{i=1}^{n}x_i(y_i-b))
  $$

  $$
  \frac{\partial{L}}{\partial{b}}=2(nb-\sum_{i=1}^{n}(y_i-wx_i))
  $$

- 梯度下降
  $$
  w=w-\alpha\frac{\partial{L}}{\partial{w}}
  $$

  $$
  b=b-\alpha\frac{\partial{L}}{\partial{b}}
  $$

## 3、逻辑斯蒂回归（用回归的思想去解决分类问题）

### 3.1 定义

逻辑斯蒂回归(Logistic Regression)虽然名字中有回归，但模型最初是为了解决二分类问题。线性回归模型帮助我们用最简单的线性方程实现了对数据的拟合，但只实现了回归而无法分类。因此LR就是在线性回归的基础上，构建的一种分类模型。

**对线性模型进行二分类任务**，简单的是通过阶跃函数，即将线性模型的输出值套上一个函数进行分割，大于z的判定为0，小于z的判定为1

![img](F:\面试\图片\激活函数.png)

**但是该分段函数数学性质不好，既不连续也不可微**。因此有人提出了对数几率函数，简称Sigmoid函数。
$$
y=\frac{1}{1+e^{-z}}
$$
该函数具有很好的数学性质，既可以用于预测类别，并且任意阶可微，因此可用于求解最优解，将线性函数带进去，可得LR模型为：
$$
y=\frac{1}{1+e^{-w^Tx+b}}
$$

### 3.2 损失函数

- 什么是极大似然估计：通俗理解来说，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即："模型已定，参数未知"
  - 极大似然估计中采样需要满足一个重要的假设，就是所有的采样都是独立同分布的。

y为概率p，根据似然函数的对数可得
$$
L=-[ylogy_{pre}+(1-y)log(1-y_{pre})]
$$
极大似然估计推导损失函数，写出似然函数：
$$
P(y^{(i)}=1|x^{(i)};\theta)=h_{\theta}(x^{(i)})
$$

$$
P(y^{(i)}=0|x^{(i)};\theta)=1-h_{\theta}(x^{(i)})
$$

$$
L=\Pi_{i=1}^mP(y^{(i)}|y^{(i)}_{pre})
$$

$$
=\Pi_{i=1}^my_{pre}^{(i)y^{(i)}}(1-y_{pre}^{(i)})^{1-y^{(i)}}
$$

取对数
$$
logL=\sum_{i=1}^my^{(i)}logy_{pre}^{(i)}+(1-y^{(i)})log(1-y_{pre}^{(i)})
$$
根据**梯度下降法**求解，需要**链式求导法则求解**
$$
\frac{\partial{J(\theta)}}{\partial{\theta_j}}=\frac{\partial}{\partial{\theta}}(\sum_{i=1}^m[y^{(i)}log(h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))])
$$

$$
=\sum_{i=1}^m[y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}-(1-y^{(i)})\frac{1}{1-h_{\theta}(x^{(i)})}]\frac{\partial}{\partial{\theta}}h_{\theta}(x^{(i)})
$$

$$
=\sum_{i=1}^m[y^{(i)}\frac{1}{h_{\theta}(x^{(i)})}-(1-y^{(i)})\frac{1}{1-h_{\theta}(x^{(i)})}]h_{\theta}(x^{(i)})(1-h_{\theta}(x^{(i)}))\frac{\partial}{\partial{\theta}}\theta{x^{(i)}}
$$

$$
=\sum_{i=1}^m[y^{(i)}(1-h_{\theta}(x^{(i)}))-(1-y^{(i)})h_{\theta}(x^{(i)})]\frac{\partial}{\partial{\theta}}\theta{x^{(i)}}
$$

$$
=\sum_{i=1}^m[y^{(i)}-h_{\theta}(x^{(i)})]x_j^{(i)}
$$

### 3.3 逻辑斯蒂回归的优缺点

- 优点
  - 将推荐问题转化为CTR点击率预估问题，**能综合利用用户、物品、上下文等多种不同的特征**
  - 模型简单，可解释性强
  - 训练开销小
    - 简单的线性计算
    - 训练时便于并行化，在预测时只需要对特征进行线性加权，所以性能比较好
  - 资源占用小，尤其是内存：在实际工程中只需要存储权重比较大的特征及特征对应的权重
- 缺点
  - 表达能力不足，**无法进行特征交叉，特征筛选等一系列高级操作**
  - **准确率并不是很高**：模型太过简单，很难去拟合数据的真实分布

## 4、决策树

一颗完整的决策树包含以下三个部分：

- **根节点：**就是树最顶端的节点，即初始特征
- **叶子节点：**树最底部的节点，也就是决策结果
- **内部节点：**除了叶节点，都是内部节点

决策树采用的是自顶向下的递归方法，**其基本思想是以信息熵为度量构造一棵熵值下降最快的树**，到叶子节点的熵值为0，此时每个叶子节点的实例都属于同一类。

### 4.1 信息论知识

#### 4.1.1 信息熵概念

离散型随机变量X的取值为X1、X2、...、Xn，发生概率分别为P1、P2、...、Pn，则信息熵为：
$$
H(X)=-\sum_{i=1}^np_ilog(p_i)
$$
**信息熵用于描述信息的不确定度，概率越大，可能性越大，信息量越小，不确定度越小，熵越小。**

#### 4.1.2 条件熵

设随机变量(X, Y)具有联合概率分布：
$$
P(X=x_i,Y=y_i)=p_{ij}
$$
条件熵H(Y|X)表示在**已知随机变量X的条件下随机变量Y的不确定性。**

(X, Y)发生所包含的熵，减去X单独发生的熵，就是在X发生的前提下，Y发生带来的熵
$$
H(Y|X)=H(X,Y)-H(X)
$$

$$
H(X,Y)-H(X)=-\sum_{x,y}p(x,y)logp(x,y)+\sum_xp(x)
		   =-\sum_{x,y}p(x,y)logp(x,y)+\sum_x(\sum_yp(x,y))logp(x)
		   =-\sum_{x,y}p(x,y)logp(x,y)+\sum_{x,y}p(x,y)logp(x)
		   =-\sum_{x,y}p(x,y)logp(y|x)
$$

#### 4.1.3 相对熵

![img](G:\面试\机器学习\决策树.png)

### 4.2 ID3算法

核心是在决策树各个节点上应用**信息增益**准则选择特征，递归地构建决策树。

**具体方法是：**从根结点开始，对节点计算所有可能特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点，再对子节点递归的调用以上方法，构建决策树；**直到所有特征的信息增益均很小或没有特征可以选择为止**。

ID3相当于用**极大似然法**进行概率模型的选择。使用**二分法则易于对树构建过程中进行调整以处理连续性特征**。**具体的处理方法是：如果特征值大于给定值就走左子树，否则走右子树。**

**信息增益：**给定一个样本集D，划分前样本集合D的熵是一定的，用H0表示，使用某个特征A划分数据集D，计算划分后的数据子集的熵，用H'表示。
$$
信息增益=H_0-H'
$$

$$
H_0=-\frac{5}{14}log\frac{5}{14}-\frac{9}{14}log\frac{9}{14}=0.9403
$$

选天气特征：
$$
H_1=-\frac{2}{5}log\frac{2}{5}-\frac{3}{5}log\frac{3}{5}=0.9710
$$

$$
H_2=-log1=0
$$

$$
H_3=-\frac{2}{5}log\frac{2}{5}-\frac{3}{5}log\frac{3}{5}=0.9710
$$

$$
H'=\frac{5}{14}H_1+\frac{4}{14}H_2+\frac{5}{14}H_3=0.6936
$$

$$
信息增益=0.9403-0.6936=0.2467
$$

**局限：信息增益偏向取值较多的特征**

**原因：**当**特征的取值较多时**，根据此特征划分更容易得到纯度更高的子集，因此划分之后的熵更低，由于划分前的熵是一定的，因此信息增益更大，**因此信息增益比较偏向取值较多的特征。**（比如ID类特征，如果根据ID划分的话，每个人的ID都是不同的，那么每个子集的熵都为0，那么信息增益就很大了。）

### 4.3 C4.5算法

算法用**信息增益率**选择特征，在树的构造过程中会进行剪枝操作优化，能够自动完成对连续属性的离散化处理；**在选择分割特征时选择信息增益率最大的特征。**

为了解决信息增益的局限，引入了信息增益率的概念。分支过多容易导致过拟合，造成不理想的后果。**定义决策指标=信息增益/特征本身的熵。**
$$
H_0=0.9403,H_1=0.9710,H_2=0,H_3=0.9710
$$

$$
H'=\frac{5}{14}H_1+\frac{4}{14}H_2+\frac{5}{14}H_3=0.6936
$$

$$
H_0'=-(\frac{5}{14}log\frac{5}{14}+\frac{4}{14}log\frac{4}{14}+\frac{5}{14}log\frac{5}{14})=1.5774
$$

$$
信息增益率=H'/H_0'=0.1566
$$

**既然信息增益可以计算，为什么C4.5还使用信息增益率：**

在使用信息增益的时候，如果某个特征有很多取值，使用这个取值多的特征会得到大的信息增益，这个问题是出现很多分支，将数据划分更细，模型复杂度更高，出现过拟合的几率更大。**使用信息增益比就是为了解决偏向于选择取值较多的特征的问题。使用信息增益率对取值多的特征加上的惩罚，对这个问题进行校正。**

**信息增益率本质：**是在信息增益的基础上乘上一个惩罚系数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。

### 4.4 Gini系数和CART算法

**使用Gini作为分割属性选择的标准，选择Gini最大的作为当前数据集的分割属性。**

Gini：**表示在样本集合中一个随机选中的样本被分错的概率**

**Gini指数越小表示集合中被选中的样本被分错的概率越小，也就是说集合的纯度越高，反之，集合越不纯。**

Gini指数（Gini不纯度）=样本被选中的概率*样本被分错的概率
$$
Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2
$$

- Pk表示选中的样本属于k类别的概率，被错分的概率为1-Pk
- 样本集合中有K个类别，一个随机选中的样本可属于这K个类别中的任意一个
- 当样本属于每一个类别的概率都相等即均为1/k时，Gini系数最大，不确定度最小

**CART决策树又称分类回归树。当CART是分类树时，采用Gini值作为结点分裂的依据；当CART为回归树时，采用MSE作为结点分裂的依据**。

CART算法由以下两步组成：

- **决策树生成**：基于训练数据集生成决策树，生成的决策树要尽量大
- **决策树剪枝**：**用验证数据集对已生成的树进行剪枝并选择最优子树**，这时用损失函数最小作为剪枝的标准。

#### 4.4.1 Gini指数和信息熵都表示数据不确定性，为什么CART使用Gini指数

信息熵logK都是值越大，数据的不确定性越大。**信息熵需要计算对数，计算量大；信息熵是可以处理多个类别，Gini指数就是针对两个类计算的，由于CART树是一个二叉树，每次都是选择yes or no进行划分**，从这个角度也是应该选择简单的Gini指数进行计算。

#### 4.4.2 Gini指数存在的问题？

**Gini指数偏向于多值属性**。当类数较大时，Gini指数求解比较困难；Gini指数倾向于支持在两个分区中生成大小相同的测试

### 4.5 ID3 vs C4.5 vs CART

|                      | ID3                                                          | C4.5                                                         | CART                          |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------------------- |
| 分叉情况             | 多叉树                                                       | 多叉树                                                       | 二叉树                        |
| 应用类型             | 分类树                                                       | 分类树                                                       | 分类和回归树                  |
| 生成过程特征是否复用 | 不复用                                                       | 不复用（离散型特征）和复用（连续型特征）                     | 复用                          |
| 特征选择方式         | 信息增益                                                     | 信息增益比                                                   | 分类树：Gini<br />回归树：MSE |
| 优点                 |                                                              | 产生的规则易于理解；准确率较高；实现简单                     |                               |
| 缺点                 | 1、不能对连续数据进行处理，只能通过连续数据离散化进行处理<br />2、采用信息增益进行数据分裂容易偏向取值较多的特征，准确性不如信息增益率<br />3、缺失值不好处理<br />4、没有采用剪枝，决策树的结构可能过于复杂，出现过拟合 | 1、对数据进行多次顺序扫描和排序，效率较低<br />2、只适合小规模数据集，需要将数据放到内存中 |                               |

| C4.5算法改进 | 将连续的特征离散化，取相邻两样本值的平均值，其中第i个划分点  |
| ------------ | ------------------------------------------------------------ |
|              | 采用信息增益率的方法，它是信息增益和特征熵的比值，特征数较多的特征对应的特征熵越大，它作为分母，可以校正信息增益偏向取值较多的特征的问题 |
|              | 主要需要解决的是两个问题<br />一是在样本某些特征缺失的情况下选择划分的属性<br />对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重，然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2，然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例 |
|              | 二选定了划分属性，对于在该属性上缺失特征的样本的处理<br />对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点。不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征A1，A2，A3。3个特征值对应的无缺失A特征的样本个数为2，3，4。则a同时划分入A1，A2，A3。对应权重调节为2/9，3/9，4/9 |
|              | 引入了正则化系数进行初步的剪枝，剪枝有两种：<br />预剪枝：在构造过程中，当某个节点满足剪枝条件，则直接停止此分支的构造<br />后剪枝：先构造完成完整的决策树，再通过某些条件遍历树进行剪枝 |

### 4.6 决策树

定义：决策树就是一棵树，其中根节点和内部节点是输入特征的判定条件，叶子节点就是最终结果。

**其损失函数通常是正则化的极大似然函数；**

目标：**以损失函数为目标函数的最小化。**

算法通常是一个递归的选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类过程。

判断数据集'纯'的指标有三个：

- Gini指数
- 熵
- 错误率

#### 4.6.1 决策树的数据split原理或者流程

- 将所有样本看作一个节点
- 根据纯度量化指标，**计算每一个特征的纯度，根据最不纯的特征进行数据划分**
- 重复上述步骤，**直到每一个叶子节点都足够纯或者达到停止条件**

#### 4.6.2 构造决策树的步骤

- 特征选择
- 决策树的生成（包括预剪枝）——只考虑局部最优
- 决策树的剪枝（后剪枝）——只考虑全局最优

#### 4.6.3 决策树算法中如何避免过拟合和欠拟合

- 过拟合：
  - 选择能够反映业务逻辑的训练集去产生决策树；
  - 剪枝操作（预剪枝和后剪枝）；
  - k折交叉验证

- 欠拟合：
  - 增加树的深度；
  - 随机森林RF


#### 4.6.4 决策树怎么剪枝

- **预剪枝**：在构造决策树的过程中加入限制，比如**控制叶子节点最少的样本个数、树的最大深度，提前停止**；
- **后剪枝**：在决策树构造完成之后，根据**加上正则化的结构风险最小化自下向上进行的剪枝操作**。损失函数加上惩罚项（**叶子节点个数**）

**剪枝的目的是防止过拟合，是模型在测试数据上表现良好，增强鲁棒性**

#### 4.6.5 决策树的优缺点

- 优点
  - 决策树模型可读性好，具有描述性，有助于人工分析
  - 效率高，决策树只需要一次性构建，反复使用，每一次预测的最大计算次数不超过决策树的深度

- 缺点：
  - 即使做了预剪枝，它也经常过拟合，泛化性能很差
  - 对中间值的缺失敏感
  - ID3算法计算信息增益时结果偏向数值多的特征

#### 4.6.6 如果特征很多，决策树中最后没有用到的特征一定无用吗？

不是。从两个角度考虑：

- 一是特征替换性，如果已经使用的特征A和特征B可以替代特征C，特征C可能就没有被使用，但是如果把特征C单独拿出来训练，依然有效。
- 二是决策树的每一条路径就是计算条件概率的条件，前面的条件如果包含了后面的条件，只是这个条件在这颗树中是无用的，如果把这个条件拿出来也是可以帮助分析的。

#### 4.6.7 决策树怎么做回归

给回归定义一个损失函数，比如L2损失，可以把分叉结果量化；最终的输出值是分支下的样本均值

#### 4.6.8 决策树算法的停止条件

- **最小节点数**：当节点的数据量小于一个指定的数量时，不继续分裂。两个原因：一是数据量少时，再做分裂容易强化噪声数据的作用；二是降低树生长的复杂性。提前结束分裂一定程度上有利于降低过拟合的影响。
- **熵或Gini值小于阈值**：当熵或Gini值过小时，表示数据的纯度比较大，如果熵或者Gini值小于一定程度，节点停止分裂
- **决策树的深度达到指定的条件**：决策树的深度是所有叶子节点的最大深度，当深度到达指定的上限大小时，停止分裂
- **所有特征已经使用完毕**，不能继续进行分裂

## 5、随机森林

**定义**：随机森林就是通过**集成学习的思想**把多棵树集成的一种算法，它的基本单元是**决策树**，**而它的本质属于集成学习方法。**它的工作原理是生成多个分类器模型，各自独立地学习和作出预测。这些预测最终结合成单预测，因此优于一个单分类做出的预测。

算法思想：随机选择样本（放回抽样）——>随机选择特征——>构建决策树——>随机森林投票（平均）

| 优点                                                         | 缺点                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 并行                                                         | 在解决回归问题时，表现较差，这是因为它并不能给出一个连续的输出 |
| 随机性的引入，增加了多样性，泛化能力非常强，抗噪能力强，对缺失值不敏感 | 在某些噪声较大的分类或者回归问题上会过拟合                   |
| 可省略交叉验证，因为随机采样                                 | 对许多统计建模者来说，无法控制模型内部运行（可控性差）       |
| 可得到特征重要性排序，因此可做特征选择                       | 对于特征较少的数据，可能不能产生很好的分类                   |
| 可处理高维特征，且不用特征选择                               | 可能有很多相似的决策树，掩盖了真实的结果                     |
| 能处理离散型、连续型数据，无需规范化                         | 执行速度比boosting等快，但比单个决策树慢得多                 |

### 5.1 随机森林的随机性指的是？

- 决策树**训练样本是有放回随机采样**的
- 决策树节点分裂**特征集是有放回随机抽样**的

### 5.2 为什么随机抽样？

保证基分类器的多样性，若每棵树的样本集都一样，训练的每颗决策树都会一样。

### 5.3 为什么有放回地抽样

保证样本集之间有重叠，若不放回，**每个训练集及其分布都不一样，容易导致训练的各个决策树差异性很大，最终多数表决无法求同，即最终多数表决相当于求同过程。**

### 5.4 为什么不用全样本训练

**全样本忽视了局部样本的规律**，不利于模型的泛化能力。

### 5.5 为什么要随机特征？

**随机特征保证基分类器的多样性（差异性）**，最终集成的泛化性能可通过学习器之间的差异度而进一步提升，**从而提高泛化能力和抗噪能力。**

### 5.6 需要剪枝吗？

不需要，后剪枝是为了避免过拟合，随机森林随机选择变量与树的数量，已经避免了过拟合，没必要去剪枝了。一般RF要控制的是树的规模，而不是树的置信度，剩下的每棵树需要做的就是尽可能的在自己所对应的数据集下尽可能的做到最好的预测结果。

剪枝的作用其实被集成方法代替的，所以用处不大。

### 5.7 随机森林如何处理缺失值

- 对于训练集，同一个类下的数据：如果是分类变量缺失，用众数补上；如果是连续型变量缺失，用中位数补
- 先用上述方法补上缺失值，然后构建森林并计算相似矩阵，再回头看缺失值，如果是分类变量，则用加权平均的方法补缺失值。迭代4-6次。

### 5.8 随机森林如何评估特征的重要性？

如果一个变量足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是很重要

### 5.10 RF与决策树的区别

- RF是决策树的集成
- RF中是随机属性型决策树

### 5.11 RF为什么比bagging效率高

因为在个体决策树的构建过程中，Bagging使用的是'确定型'决策树，bagging在选择划分属性时要对每棵树是对所有特征进行考察

而随机森林仅仅考虑一个特征子集

### 5.12 RF为什么更鲁棒

由于RF使用了行采样和列采样技术，使得每棵树不容易过拟合；并且是基于树的继承算法，由于使用了随机数据使得每棵树差别较大，在进行embedding的时候，**可以更好地降低模型的方差，整体而言使得RF是一个鲁棒的模型。**

### 5.13 RF分类和回归问题如何预测y值

RF是一个**加权平均**的模型，**进行分类问题的时候，使用的k个树的投票策略：少数服从多数**。**在回归问题是使用的k个树的平均**。可以看出RF的训练和预测过程都可以进行并行处理。

### 5.14 为什么RF的树比GBDT的要深

RF是通过投票的方式来降低方差，**但是本质上需要每棵树有较强的表达能力，所以单颗树深点没关系，通过投票的方式降低过拟合**。而GBDT是通过加强前一棵树的表达能力，所以每棵树不必有太强的表达能力。可以通过boosting的方式来提高，也能提高训练速度（**GBDT害怕过拟合，RF不怕，通过投票的方式杜绝**）。

## 6、KNN

![IMG_256](G:\面试\机器学习\KNN.png)

### 6.1 简述KNN算法的原理（分类算法）

利用训练数据集对特征向量空间进行划分。**KNN算法的核心思想是在一个含未知样本的空间，可以根据样本最近的k个样本的数据类型来确定未知样本的数据类型。**

该算法涉及的3个主要因素是：**k值选择，距离度量，分类决策**

- 通用步骤
  - 计算距离（常用欧几里得距离或马氏距离）
  - 升序排列
  - 取前K个
  - 加权平均

### 6.2 如何理解KNN中的k的取值

在应用中，k值一般取比较小的值，并采用交叉验证法进行调优

K的选取：

- K太大：导致分类模糊。相当于用较大的领域中的训练实例进行预测，减少测试误差，增大训练误差。K值增大意味着整体模型变简单，**容易欠拟合**。
- K太小：受个例影响，波动较大。相当于用较小的领域中的训练实例进行预测，训练误差会减小，只有与输入实例较近的训练实例才会对预测结果起作用，测试误差会增大。K值减小意味着整体模型变复杂，**容易过拟合。**

如何选取K：

- 经验
- 均方根误差

### 6.3 在KNN的样本搜索中，如何进行高效的匹配查找

线性扫描（数据多时效率很低）构建数据索引——clipping和overlapping两种；或者划分的空间没有重叠，如k-d树；后者划分的空间相互重叠，如R树。

### 6.4 KNN算法的优点和缺点

| 优点                                          | 缺点                         |
| --------------------------------------------- | ---------------------------- |
| 既可以做分类也可以做回归                      | 计算量大                     |
| 可以用于非线性分类/回归；训练时间复杂度为O(n) | 存在类别不平衡问题           |
| 准确率高，对数据没有假设，对离群点不敏感      | 需要大量的内存，空间复杂度高 |

### 6.5 不平衡的样本可以给KNN的预测结果造成哪些问题，有没有好的解决方法

输入实例的K近邻点中，大数量类别的点会比较多，但其实可能都离实例较远，这样会影响最后的分类。

可以使用权值来改进，距离实例较近的点赋予较高的权重，较远的点赋予较低的权重

### 6.6 为了解决KNN算法计算量过大的问题，可以使用分组的方法进行计算，简述一下该方法的原理

先将样本按距离分解成组，获得质心，然后计算未知样本到各质心的距离，选出距离最近的一组或几组，再在这些组内引用KNN。

本质上就是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本，该方法比较适用于样本容量比较大时的情况。

### 6.7 如何优化K-Means

使用k-d树或者Ball Tree：将所有的观测实例构建成一颗kd树，之前每一个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可。

### 6.8 在K-means或KNN，我们是用欧式距离来计算最近的邻居之间的距离，为什么不用曼哈顿距离？

曼哈顿距离只计算水平或垂直距离，有维度的限制。另一方面，欧式距离可用于任何空间的距离计算问题。

![img](G:\面试\机器学习\距离.jpg)

绿色的线为欧氏距离的丈量长度，红色的线即为曼哈顿距离长度。

蓝色和黄色的线是这两点间曼哈顿距离的等价长度。

- 欧式距离：两点之间的最短距离
  $$
  L=\sqrt{\sum_{k=1}^{n}(x_1-x_2)^2}
  $$

- 曼哈顿距离：投影到坐标轴的长度之和
  $$
  L=\sum_{k=1}^{n}|x_1-x_2|
  $$

- 切比雪夫距离：各坐标数值差的最大值
  $$
  L=max(|x_1-x_2|)
  $$

## 7、K-Means

聚类是一种无监督的学习，它将相似的对象归到一个簇中，将不相似对象归到不同簇中。

相似这一概念取决于所选择的相似度计算方法。

K-Means是发现给定数据集的K个簇的聚类算法，之所以称之为**K-均值**是因为它可以发现K个不同的簇，且每个簇的中心采用簇中所含值的均值计算而成。簇个数K是用户指定的，每一个簇通过其质心，即簇中所有点的中心来描述。

聚类与分类算法最大区别在于，分类的目标类别已知，而聚类的目标类别是未知的。

- 优点：易于实现
- 缺点：可能收敛到局部最小值，在大规模数据集上收敛较慢。

### 7.1 K-Means术语

- 簇：所有数据点的集合，簇中的对象是相似的
- 质心：**簇中所有点的中心**（计算所有点的均值而来）
- SSE：Sum of Sqared Error（平方误差和），SSE值越小，表示越接近它们的质心，由于对误差取了平方，因此更注重远离中心的点。

![image-20220331142326607](G:\面试\机器学习\K-means.png)

### 7.2 K-means工作流程

- 首先，随机确定K个初始点作为质心（不是数据中的点）
- 然后将该数据集中的每个点分配到一个簇中，具体来讲，就是为每个点找到距其最近的质心，并将其分配该质心所对应的簇。这一步完成之后，每个簇的质心更新为该簇所有点的平均值。

## 8、SVM

### 8.1 SVM概念

![image-20220331193113816](G:\面试\机器学习\SVM.png)

我们**的目标**就是从计算和数学的角度去找到这个**最大化间隔的解**，即找到最佳的直线来分割分类点，即**具有很强的鲁棒性**。

定义：**SVM是一种二分类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化**，最终转化为一个凸二次规划问题的求解。

### 8.2 线性分类器

给定一些数据点，其分别属于不同的两个类，现在要找到一个分类器把这些数据分成两类。X表示数据，Y表示类别（1，-1），一个线性分类器的目标是要在n维数据空间中找到一个超平面：
$$
W^TX+b=0
$$
$$
W^TX_m+b=1,       (1)
$$

$$
W^TX_n+b=-1,(2)
$$

$$
(1)-(2)=W^T(X_m-X_n)=2,(3)
$$

$$
W^TX_O+b=0,(4)
$$

$$
W^TX_P+b=0,(5)
$$

$$
(4)-(5)=W^T(X_O-X_P)=0,即W与决策超平面垂直
$$

$$
(3)=||W||·||X_m-X_n||·cos\theta=2
$$

$$
=||W||·L=2
$$

$$
几何间隔L=\frac{2}{||W||}
$$

Logistic回归目的是从特征中学习出一个0-1分类模型，故使用logistic函数：
$$
h_\theta(x)=\frac{1}{1+e^{-\theta^TX}}
$$

$$
P(y=1|x)=h_\theta(x),P(y=0|x)=1-h_\theta(x)
$$

### 8.3 函数间隔和几何间隔

- 函数间隔：一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面wx+b=0确定的情况下，|wx+b|能够相对地表示点距离超平面的远近。wx+b的符号与类标记y的符号是否一致能够表示分类是否正确。所以可以用y（wx+b）来表示分类的正确性及确信度，这就是函数间隔的概念。

$$
\gamma=y(w^Tx+b)=yf(x)
$$
- 几何间隔：

$$
\gamma'=\frac{\gamma}{||w||}
$$

- 当选择分离超平面的时候，只有函数间隔还不够。因为只要成比例地改变，平面并没有改变，但函数间隔却成为了原来的两倍。所以可以对分离超平面的法向量加某些约束，如规范化，使得间隔是确定的。这时函数间隔成为几何间隔。

### 8.4 最大间隔分类器的定义

对一个数据点进行分类，当**超平面离数据点的间隔越大，分类的确信度越大。**

**函数间隔不适合用来最大化间隔值，因为在超平面固定后，可以等比例地缩放w和b，这样可以使f(x)=w^Tx+b的值任意大，即γ可在超平面不变的情况下取得无限大，而几何间隔只随着超平面的变化而变化。**

目标函数：
$$
max\gamma'=max\frac{\gamma}{||w||}=max\frac{1}{||w||}
$$

### 8.5 从线性可分到线性不可分

#### 8.5.1 从原始问题到对偶问题的求解

**约束条件：**这样强制数据点不会落在分界线上。（软间隔SVM问题）
$$
s.t.  y_i(w^Tx_i+b)>=1-\sigma
$$
**可在1之后减去一个惩罚系数**，来进行平衡，因为数据中免不了包含噪声。

**目标函数：**
$$
max\frac{1}{||w||}
$$
**转换为：**
$$
min\frac{1}{2}||w||^2+C\sum_{i=1}^n\sigma_i
$$
**故损失函数可写为：**
$$
L(w,b,\alpha)=\frac{1}{2}||w||^2+C\sum_{i=1}^{n}\sigma_i-\sum_{i=1}^{n}\alpha_i(y_i(w^Tx_i+b)-1+\sigma_i)
$$

$$
\theta(w)=maxL(w,b,\alpha),\alpha_i>=0
$$

**约束条件**：硬间隔
$$
y_i*(w·x_i+b)-1>=0
$$
增加一个非负变量
$$
y_i*(w·x_i+b)-1=p_i^2
$$
损失函数：
$$
L(w,b,\alpha,p_i)=\frac{1}{2}||w||^2-\sum_{i=1}^n\alpha_i*(y_i*(w·x_i+b)-1-p^2_i)
$$
**KKT条件：**
$$
\frac{\partial{L}}{\partial{w}}=0=>w-\sum_{i=1}^n\alpha_iy_ix_i=0
$$

$$
\frac{\partial{L}}{\partial{b}}=0=>-\sum_{i=1}^n\alpha_iy_i=0
$$

$$
\frac{\partial{L}}{\partial{\alpha_i}}=0=>y_i*(w·x_i+b)-1-p_i^2=0
$$

$$
\frac{\partial{L}}{\partial{p_i}}=0=>2\alpha_ip_i=0=>\alpha_ip_i^2=0
$$

$$
\alpha_i(y_i*(w·x_i+b)-1)=0
$$

$$
\alpha_i>=0
$$

有两种情况：

- $$
  y_i*(w·x_i+b)-1>0,\alpha_i=0
  $$

- $$
  y_i*(w·x_i+b)-1=0,\alpha_i!=0
  $$

  ![image-20220425200149495](G:\面试\机器学习\lambda大于0.png)

**KKT对偶条件：**

- **所有原变量和对偶变量的梯度是0**
- 原问题中的原始的约束条件要被满足
- 要求对偶变量的约束要被满足
- **松弛互补：对所有的不等式约束，要么KKT乘子为0，要么不等式约束取等号**

$$
minmaxL(w,b,\alpha)=>maxminL(w,b,\alpha)
$$

**对偶的好处：**

- 对偶的问题往往更容易求解
- 可以**自然地引入核函数，推广到非线性分类问题**

**求解：**
$$
\frac{\partial{L}}{\partial{w}}=0=>\sum_{i=1}^{n}\alpha_iy_ix_i=w
$$

$$
\frac{\partial{L}}{\partial{b}}=0=>\sum_{i=1}^{n}\alpha_iy_i=0
$$

$$
L(w,b,\alpha)=\frac{1}{2}||w||^2-\sum_{i=1}^{n}\alpha_i(y_i(w^Tx_i+b)-1)
$$

$$
=\frac{1}{2}w^Tw-\sum_{i=1}^{n}\alpha_iy_iw^Tx_i-\sum_{i=1}^{n}\alpha_iy_ib+\sum_{i=1}^{n}\alpha_i
$$

$$
=\frac{1}{2}w^T\sum_{i=1}^{n}\alpha_iy_ix_i-w^T\sum_{i=1}^{n}\alpha_iy_ix_i-b\sum_{i=1}^{n}\alpha_iy_i+\sum_{i=1}^{n}\alpha_i
$$

$$
=-\frac{1}{2}w^T\sum_{i=1}^{n}\alpha_iy_ix_i-b\sum_{i=1}^{n}\alpha_iy_i+\sum_{i=1}^{n}\alpha_i
$$

$$
=-\frac{1}{2}(\sum_{i=1}^{n}\alpha_iy_ix_i)^T\sum_{i=1}^{n}\alpha_iy_ix_i-b\sum_{i=1}^{n}\alpha_iy_i+\sum_{i=1}^{n}\alpha_i
$$

$$
=-\frac{1}{2}\sum_{i=1}^{n}\alpha_iy_ix_i^T\sum_{i=1}^{n}\alpha_iy_ix_i-b\sum_{i=1}^{n}\alpha_iy_i+\sum_{i=1}^{n}\alpha_i
$$

$$
=\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_jx_i^Tx_j
$$

$$
w=\sum_i\alpha_iy_ix_i,b=y_i-\sum_j\alpha_jy_jx_j^Tx_i
$$

#### 8.5.2 核函数

原损失函数
$$
L(w,b,\alpha)=\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_jx_i^Tx_j
$$
使用核技巧后
$$
L(w,b,\alpha)=\sum_{i=1}^{n}\alpha_i-\frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_jT(x_i)T(x_j)
$$
使得
$$
T(x_i)T(x_j)=K(x_i,x_j)
$$
核函数的一般表达式如下
$$
K(x_i,x_j)=(c+x_i·x_j)^d
$$
高斯核函数（RBF）
$$
K(x_i,x_j)=e^{-\gamma||x_i-x_j||^2}
$$
在小γ值的情况下，数据点之间的相似度被放大了，这能让数据点更容易被简单超平面划分。在大γ情况下，除了数据点非常近的情况下，其余数据点均与其他点缺乏相似性，在计算分离超平面的过程中，需要考虑到这些点各自的空间特征，容易过拟合。

### 8.6 问题总结

#### 8.6.1 SVM的原理是什么

SVM是一种二类分类模型。它的**基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。**（间隔最大是它有别于感知机）

- 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；
- 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机
- 当训练数据线性不可分时，通过使用**核技巧及软间隔最大化**，学习非线性支持向量机。

注：以上各SVM的数学推导应该熟悉：**硬间隔最大化（几何间隔）——学习的对偶问题——软间隔最大化（引入松弛变量）——非线性支持向量机（核技巧）**

#### 8.6.2 SVM为什么采用间隔最大化

当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开，感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。**线性可分支持向量机利用间隔最大化求得最优分离超平面，这时解是唯一的。**

**另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。**可以借此机会阐述一下几何间隔以及函数间隔的关系。

#### 8.6.3 为什么SVM要引入核函数

原始空间线性不可分，可以使用一个非线性映射将原始数据x变换到一个高维特征空间，在这个空间中，样本变得线性可分。

解决方法：常用的一般是**径向基RBF函数（线性核、高斯核、拉普拉斯核等）**

#### 8.6.4 为什么SVM对缺失数据敏感

这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM没有处理缺失值的策略。**而SVM希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。**

#### 8.6.5 SVM核函数之间的区别

一般选择线性核和高斯核，也就是线性核和RBF核

本质：将每一个样本点映射到一个无穷维的特征空间

高斯核升维的本质，使得线性不可分的数据线性可分

| 模型          | 线性核                 | RBF核                                                        |
| ------------- | ---------------------- | ------------------------------------------------------------ |
| 区别          | 用于线性可分           | 用于线性不可分                                               |
|               | 参数少，速度快         | 参数多，分类结果非常依赖于参数；很多人通过训练数据的交叉验证来寻找合适的参数，过程比较耗时 |
| feature的数量 | 很大，跟样本数量差不多 | 比较小，样本数量一般                                         |

#### 8.6.6 SVM如何处理多分类问题

- 直接法，直接在目标函数上修改，将多个分类面的参数求解合并到一个最优化问题里面。计算量很大
- 间接法：对训练器进行组合。其中比较典型的有一对一和一对多。
  - 一对多，就是对每个类都训练出一个分类器，由于svm是二分类，所以将此分类器的两类设定为目标类为一类，其余类为另一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，哪个分类器的概率高，那么这个样本就属于哪一类。这种方法不太好，bias比较高。
  - 一对一法，针对任意两个类训练出一个分类器，如果有k类，一共训练出C(2,k)个分类器，这样当有一个新的样本要来的时候，用这C(2,k)个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类比被认定为该样本的类。

#### 8.6.7 带核的SVM为什么能分类非线性问题

核函数的本质是两个函数的内积，而这个函数在SVM中可以表示成对于输入值的高维映射。注意核并不是直接对应映射，核只不过是一个内积。

#### 8.6.8 RBF核一定是线性可分的吗

不一定，RBF核比较难调参而且容易出现**维度灾难**，要知道无穷维概念是从泰勒展开得出的。

维度灾难：

- 现象：**随着维度的增加，分类器性能逐步上升，到达某点之后，其性能便逐渐下降**
- 原因：继续增加特征数量，即维度增加。随着维度的增加，样本会变得越来越稀疏，在这种情况下，也更容易找到一个超平面将目标分开。但是高维空间训练形成的分类器，相当于在低维空间的一个复杂的非线性分类器，**这种分类器过多的强调了训练集的准确率甚至于对一些错误/异常的数据也进行了学习，而正确的数据却无法覆盖整个特征空间。为此，这样得到的分类器在对新数据进行预测时将会出现错误，这种现象称之为过拟合，同时也是维度灾难的直接体现。**

#### 8.6.9 常用核函数及核函数的条件

- 线性核：主要用于线性可分的情况
- 多项式核
  - **RBF核径向基**：这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核
  - 傅里叶核
  - 样条核
  - sigmoid核函数

#### 8.6.10 为什么要把求解SVM的原始问题转换为对偶问题

- 对偶问题将原始问题中的约束转为了对偶问题中的等式约束
- 方便核函数的引入
- **改变了问题的复杂度。**由求特征向量w转化为比例系数a，在原始问题下，求解的复杂度与样本的维度有关，即w的维度。在对偶问题下，只与样本数量有关。**对偶问题是凸优化问题，可以进行较好的求解，SVM中就是将原问题转换为对偶问题进行求解。**

#### 8.6.11 SVM怎么输出预测概率

把SVM的输出结果当作x经过一层LR模型得到概率，其中LR的w和b参数为使得总体交叉熵最小的值。

#### 8.6.12 如何处理数据偏斜

可以对数量多的类使得惩罚系数C越小表示越不重视，相反，数量少的类惩罚系数变大。

## 9、XGBoost

### 9.1 目标函数

#### 9.1.1 加法模型

- 基学习器	

  - 回归树
    $$
    T(\theta;x_i)=W_{q(x_i)}
    $$

  - 表达式
    $$
    y=\sum_{j=1}^Mf_j(x_i)=\sum_{j=1}^{M-1}f_j(x_i)+f_j^{(M)}(x_i)
    $$

#### 9.1.2 前向分步算法

$$
y_i^{(t)}=y_i^{(t-1)}+T(\theta;x_i)=y_i^{(t-1)}+W_{q(x_i)}
$$

#### 9.1.3 目标函数推导

$$
obj=\sum_{i=1}^NL(y_i,y_{i,pre}^{(t)})+\sum_{j=1}^t\omega(f_j),\omega(f_j)表示树的复杂度
$$

$$
\omega(f_t)=\gamma{T}+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2,T为叶子节点个数，w_j为每个节点预测值
$$

$$
obj=\sum_{i=1}^NL(y_i,y_{i,pre}^{(t)})+\sum_{j=1}^{t-1}\omega(f_j)+\omega(f_t)
$$

$$
obj=\sum_{i=1}^NL(y_i,y_{i,pre}^{(t)})+\gamma{T}+\frac{1}{2}\lambda\sum_{j=1}^Tw_j^2
$$

**可以求每一个叶子节点的最小值。**
$$
obj=\gamma{T}+\sum_{j=1}^T[\sum_{i\in{I_j}}L(y_i,y_{i,pre}^{(t)})]+\frac{1}{2}\lambda{w_j}^2
$$
**二阶泰勒展开**
$$
f(x)≈f(x_0)+f'(x_0)(x-x_0)+\frac{1}{2}f''(x_0)(x-x_0)^2
$$

$$
L(y_i,y_{i,pre}^{(t)})=L(y_i,y_{i,pre}^{(t-1)}+w_j)≈L(y_i,y_{i,pre}^{(t-1)})+L'(y_i,y_{i,pre}^{(t-1)})w_j+\frac{1}{2}L''(y_i,y_{i,pre}^{(t-1)})w_j^2
$$

$$
L(y_i,y_{i,pre}^{(t-1)}+w_j)≈\gamma{T}+\sum_{j=1}^T[\sum_{i\in{I_j}}(g_iw_j+\frac{1}{2}h_iw_j^2)]+\frac{1}{2}\lambda{w_j}^2
$$

$$
=\gamma{T}+\sum_{j=1}^T[w_j\sum_{i\in{I_j}}g_i+w_j^2\sum_{i\in{I_j}}\frac{1}{2}h_i+\frac{1}{2}\lambda{w_j}^2]
$$

$$
=\gamma{T}+\sum_{j=1}^T[w_j\sum_{i\in{I_j}}g_i+\frac{1}{2}w_j^2(\sum_{i\in{I_j}}h_i+\lambda)]
$$

$$
\gamma{T}+\sum_{j=1}^T(w_jG_j+\frac{1}{2}w_j^2(\lambda+H_j)),G_j为损失函数的一阶梯度，H_j为损失函数的二阶梯度
$$

**正则化的作用：防止过拟合**

**优化目标**：
$$
(w_1^*,w_2^*,...,w_T^*)=argmin\sum_{i=1}^NL(y_i,y_{i,pre}^{(t)})+\sum_{j=1}^t\omega(f_j)
$$

#### 9.1.4 最优解

$$
w^*_j=-\frac{G_j}{H_j+\lambda}
$$

代入：
$$
obj^*≈\gamma{T}-\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda}
$$

### 9.2 构建树的方法

- 穷举法

- 贪心算法

  - 每次只计算一个结点的最优划分

  - 按照增益来决定优劣
    $$
    gain=-\gamma+\frac{1}{2}(\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{G^2}{H+\lambda})
    $$

  - 何时停止

    - 最大增益小于某个阈值
    - 叶子结点包含样本个数为一个
    - 叶子结点个数、层数

- 近似算法

  - 压缩特征数：列采样

    - 按树随机

      在根结点分裂之前就选取候选特征，一开始定下需要使用的特征。

    - 按层随机

      每一次对新的树进行划分时，重新选取特征。

  - 压缩候选切分点数量：即**减少阈值划分的边界个数**，分桶

    - 遵循原则：每个桶内的样本个数相差不会过多
    - 加权分位法
    - 策略
      - 全局策略：**分桶策略**一致
      - 局部策略：**分桶策略**每一个结点使用的策略不一致

- 缺失值处理

- 学习率shrink：防止过拟合，也叫做**收缩率**

### 9.3 系统设计

- 分块并行
- 缓存优化
- 核外块运算

## 10、GBDT

使用的基学习器是**决策树CART算法**

### 10.1 决策树CART算法

#### 10.1.1 回归树

- 数学表达式
  $$
  f(x)=\sum_{m=1}^Mc_mI(x\in{R_m})
  $$
  每个叶子结点都会输出一个预测值。**预测值一般是该叶子所含训练样本在该结点上的输出的均值。**

- 树如何构建

  - **树的深度**如何决定
    - 确定叶子节点个数或者树的深度
    - 子节点所包含样本数
    - 给定精度
  - 划分节点如何选取：不同的节点划分条件，对应着不同的树，也就对应着不同的损失，我们从中选取使得损失最小的树即可
  - 叶子节点代表的值c<sub>m</sub>如何定？

- 损失函数
  $$
  \frac{1}{n}\sum_{i=1}^n(f(x_i)-y_i)^2
  $$
  **以均方误差MSE作为损失函数**

- 优化求解

  - 结论：
    $$
    C_m=\frac{\sum_{x_i\in{R_m}}y_i}{N_m}
    $$
    即，**当每个叶子节点的Cm取值为该节点所有样本yi的平均值时**，得到损失最小即最优的回归树。

  - 推导过程

    损失函数：
    $$
    J=\frac{1}{n}\sum_{i=1}^n(f(x_i)-y_i)^2
    $$

    $$
    =\frac{1}{n}\sum_{i=1}^n(\sum_{m=1}^MC_mI(x_i\in{R_m})-y_i)^2
    $$

    $$
    =\frac{1}{n}\sum_{m=1}^M\sum_{x_i\in{R_m}}(C_m-y_i)^2
    $$

    优化目标：
    $$
    C_m^*=min\frac{1}{n}\sum_{m=1}^M\sum_{x_i\in{R_m}}(C_m-y_i)^2
    $$
    由于损失函数只包含一个参数cm，可直接对J求导并令导数等于0，进行求解。
    $$
    \frac{\partial{J}}{\partial{C_m}}=\frac{\partial{\frac{1}{n}\sum_{m=1}^M\sum_{x_i\in{R_m}}(C_m-y_i)^2}}{\partial{C_m}}=\frac{\partial{\sum_{x_i\in{R_m}}}(C_m-y_i)^2}{\partial{C_m}}
    $$

    $$
    =2\sum_{x_i\in{R_m}}(C_m-y_i)=0
    $$

    $$
    =N_mC_m-\sum_{x_i\in{R_m}}y_i=0
    $$

    $$
    C_m=\frac{\sum_{x_i\in{R_m}}y_i}{N_m}
    $$

#### 10.1.2 分类树 

- 特征选择方式

  - Gini系数最小

  - Gini系数：分类问题中，假设有K个类，样本点属于第k类的概率为Pk，则概率分布的Gini指数定义为
    $$
    Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2
    $$

    - 特殊：二分类问题，若样本属于第一个类的概率是p，则概率分布的Gini指数为：
      $$
      Gini(p)=2p(1-p)
      $$

    - 对给定的样本集合，其Gini指数为
      $$
      Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2
      $$
      这里Ck是D中属于第k类的样本子集，K是类的个数

      如果样本集合D根据特征A是否取某一可能值a被分割为D1和D2两部分，即
      $$
      D_1=[(x,y)\in{D}|A(x)=a],D_2=D-D_1
      $$
      则在特征A的条件下，集合D的Gini系数为：
      $$
      Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
      $$

- 算法流程

#### 10.1.3 区别

- **回归树**：使用的是**平方损失**
  - 特征是离散型类别特征时：是否是某一个值
  - 特征是连续性类别特征时：筛选阈值，进行分段处理
- **分类树**：使用的是**Gini系数**
  - 特征是离散型类别特征时：是否是某一个值
  - 特征是连续性类别特征时：筛选阈值，进行分段处理

### 10.2 整体概述

$$
f_M(x)=\sum_{m=1}^MT(x;\theta_m),M为树的个数，T(x;\theta_m)表示决策树，\theta_m表示决策树的参数
$$

- 加法模型：

- 损失函数：

  - 回归问题：**平方误差损失函数**
  - 分类问题
    - 二分类问题：**指数损失函数**
    - 多分类问题：**softmax**
  - 一般决策问题：**自定义损失函数**

- 优化方法：**前向分步算法**

  **提升树算法采用前向分步算法**。首先确定初始提升树f0(x)=0，第m步的模型是：
  $$
  f_m(x)=f_{m-1}(x)+T(x;\theta_m)
  $$
  其中，fm-1(x)为当前模型，通过经验风险极小化确定下一棵决策树的参数
  $$
  \theta_m^*=argmin\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\theta_m))
  $$

### 10.3 二分类问题的提升树

- 基学习器：CART回归树
- 损失函数：指数损失函数
- 相当于**AdaBoost算法的特殊情况**
  - 基分类器G(x)限制为**二类CART树**
  - 基分类器权重αm全部置为1

### 10.4 回归问题的提升树

- 基学习器：回归树
  $$
  T(x;\theta)=\sum_{j=1}^Jc_jI(x\in{R_j})
  $$

- 损失函数：平方误差损失
  $$
  L(y,f(x))=(y-f(x))^2
  $$

  $$
  =[y-f_{m-1}(x)-T(x;\theta_m)]^2
  $$

  $$
  =[r-T(x;\theta_m)]^2
  $$

  r叫做**残差**

- 前向分步算法
  $$
  \theta_m=argmin\sum_{i=1}^N(r_m^{(i)}-T(x^{(i)};\theta_m))^2
  $$

- 思路

  - 个体学习器如何训练得到？改变训练数据的权值或概率分布如何改变？：**残差**

  - 如何将个体学习器组合：**直接相加**

  - 目标：使得总体损失逐步减小

- 算法流程：输入训练数据集T=[(x1,y1), (x2,y2), ..., (xN,yN)]，输出提升数fM(x)

  - 初始化f0(x)=0

  - 对m=1,2,...,M

    - 计算残差
      $$
      r_{mi}=y_i-f_{m-1}(x_i),i=1,2,...,N
      $$

    - 拟合残差学习一个回归树

    - 更新
      $$
      f_m(x)=f_{m-1}(x)+T(x;\theta_m)
      $$

  - 得到回归问题提升树
    $$
    f_M(x)=\sum_{m=1}^MT(x;\theta_m)
    $$

### 10.5 一般决策问题的梯度提升树GBDT

- 基学习器：回归树
  $$
  T(x;\theta)=\sum_{j=1}^Jc_jI(x\in{R_j})
  $$

- 损失函数：一般损失函数
  $$
  L(y,f(x))
  $$

- 前向分步算法+梯度提升

  - 核心目标：1、已知加法模型，一定会存在多个优化器，不断迭代优化。2、我们要确保，每增加一个基学习器，都要使得总体损失越来越小，即第m步要比第m-1步的损失要小。
    $$
    L(y^{(i)},f_m(x^{(i)}))<L(y^{(i)},f_{m-1}(x^{(i)}))
    $$

  - 将损失函数进行处理，对损失函数进行**一阶泰勒展开**

    目标：
    $$
    L(y^{(i)},f_{m-1}(x^{(i)}))-L(y^{(i)},f_m(x^{(i)}))>0
    $$
    一阶泰勒展开：
    $$
    f(x)≈f(x_0)+f'(x_0)(x-x_0)
    $$
    L(y,fm(x))中只有fm(x)是未知量，且
    $$
    f_m(x)=f_{m-1}(x)+T(x;\theta_m)
    $$
    因此有：
    $$
    f(x)≈f(x_0)+f(x_0)(x-x_0)
    $$

    $$
    L(y,f_m(x))≈L(y,f_{m-1}(x))+\frac{\partial{L(y,f_m(x))}}{\partial{f_m(x)}}|_{f_m(x)=f_{m-1}(x)}(f_m(x)-f_{m-1}(x))
    $$

    $$
    ≈L(y,f_{m-1}(x))+\frac{\partial{L(y,f_m(x))}}{\partial{f_m(x)}}|_{f_m(x)=f_{m-1}(x)}T(x;\theta_m)
    $$

    即有：
    $$
    L(y,f_{m-1}(x))-L(y,f_{m}(x))≈-\frac{\partial{L(y,f_m(x))}}{\partial{f_m(x)}}|_{f_m(x)=f_{m-1}(x)}T(x;\theta_m)
    $$
    当满足以下条件时
    $$
    T(x;\theta_m)≈-\frac{\partial{L(y,f_m(x))}}{\partial{f_m(x)}}|_{f_m(x)=f_{m-1}(x)}
    $$
    满足：
    $$
    L(y^{(i)},f_{m-1}(x^{(i)}))-L(y^{(i)},f_m(x^{(i)}))>=0
    $$

    $$
    r_m(x,y)=-[\frac{\partial{L(y,f_m(x))}}{\partial{f_m(x)}}]_{f_m(x)=f_{m-1}(x)}
    $$

  - 梯度提升
    - **计算当前损失函数的负梯度表达式**
    - 构造新的训练样本
    - **让当前的基学习器去拟合上述训练样本，得到CART决策树**

- 算法流程：输入训练数据集T=[(x1,y1),(x2,y2),...,(xN,yN)]，损失函数L(y,f(x))，输出回归树f(x)

  - 初始化
    $$
    f_0(x)=argmin\sum_{i=1}^NL(y_i,c)
    $$

  - 对m=1,2,...,N

    - 对i=1,2,...,N，计算
      $$
      r_m(x,y)=-[\frac{\partial{L(y,f_m(x))}}{\partial{f_m(x)}}]_{f_m(x)=f_{m-1}(x)}
      $$

    - 对残差拟合一个回归树，得到第m棵树的叶结点区域R<sub>mj</sub>，j=1,2,...,J

    - 对j=1,2,...,J，计算
      $$
      c_{mj}=argmin\sum_{x_i\in{R_{mj}}}L(y_i,f_{m-1}(x_i)+c)
      $$

    - 更新
      $$
      f_m(x)=f_{m-1}(x)+\sum_{j=1}^Jc_{mj}I(x\in{R_{mj}})
      $$

  - 得到回归树
    $$
    f_M(x)=\sum_{m=1}^M\sum_{j=1}^Jc_{mj}I(x\in{R_{mj}})
    $$

- 思路

- GBDT优缺点

## 11、AdaBoost

解决的是**二分类问题**。

数学表达：
$$
f(x)=\sum_{m=1}^M\alpha_mG_m(x)
$$

$$
=\alpha_1G_1(x)+...+\alpha_mG_m(x)+...+\alpha_MG_M(x)
$$

思路：

- 在每一轮中，分别记录好那些被当前**弱分类器正确分类与错误分类的样本**，在下一轮训练时，**提高错误分类样本的权值，同时降低正确分类样本的权值，用以训练新的弱分类器**。这样一来，那些没有得到正确分类的数据，由于其权值加大，会受到后一轮的弱分类器的更大关注
- 加权多数表决
  - **加大分类误差率小的弱分类器的权值，使其在表决中起较大作用**
  - **减小分类误差率大的弱分类器的权值，使其在表决中起较小作用**

### 11.1 算法流程

- 获得二分类训练数据集
  $$
  T=[(x_1, y_1),(x_2,y_2),...,(x_N,y_N)]
  $$
  其中，每个样本点由实例与标记组成。实例xi，标记yi={-1，+1}。

- **定义基分类器（弱分类器）**

- 循环M次

  - 初始化、更新当前训练数据的权值分布

    - 初始化
      $$
      D1=(w_{11},...,w_{1i},...,w_{1N}),w_{1i}=\frac{1}{N},i=1,2,...,N
      $$

    - 更新
      $$
      D_m=(w_{m,1},...,w_{m,i},...,w_{m,N})
      $$

      $$
      w_{m,i}=\frac{w_{m-1,i}}{Z_{m-1}}exp(-\alpha_{m-1}y_iG_{m-1}(x_i)),i=1,2,...,N
      $$

      $$
      Z_{m-1}=\sum_{i=1}^Nw_{m-1,i}exp(-\alpha_{m-1}y_iG_{m-1}(x_i)),规范化因子
      $$

      它使得**Dm**称为一个**概率分布**
      $$
      w_{m,i}=
      \begin{cases}
      \frac{w_{m-1,i}}{Z_{m-1}}e^{-\alpha_{m-1}}, & G_{m-1}(x_i)=y_i \\ 
      \frac{w_{m-1,i}}{Z_{m-1}}e^{\alpha_{m-1}}, & G_{m-1}(x_i)≠y_i
      \end{cases}
      $$
      由此可知，**被基本分类器Gm(x)误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。**

  - 训练当前基分类器Gm(x)

    - 使用具有权值分布Dm的训练数据集学习，**得到基分类器Gm(x)**

  - 计算当前**基分类器的权值αm**

    - 计算当前Gm(x)在训练数据集上的**分类误差率**
      $$
      e_m=\sum_{i=1}^NP(G_m(x_i)≠y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i)≠y_i)=\sum_{G_m(x_i)≠y_i}w_{mi},0<=e_m<=0.5
      $$

    - 根据**分类误差率em**，计算基分类器G<sub>m</sub>(x)**的权重系数**
      $$
      \alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}
      $$

  - 将α<sub>m</sub>G<sub>m</sub>(x)更新到加法模型f(x)中

  - 判断是否满足循环退出条件

    - 分类器个数是否达到M
    - 总分类器误差率是否低于设定的精度

### 11.2 加法模型

- 预测函数
  $$
  f(x)=\sum_{m=1}^M\beta_mb(x;\gamma_m),\beta_m为基函数的系数，b(x;\gamma_m)为基函数，\gamma_m为基函数的参数
  $$
  类比于Adaboost的预测函数：**可知道AdaBoost就是一个加法模型**

- 损失函数

  - 自定义损失函数
    $$
    L(y,f(x))
    $$

  - 回归问题

    - MSE均方误差

  - 分类问题

    - 指数函数
    - 交叉熵损失

- 优化方法

  - **梯度下降**的缺点

    - 整体损失极小化
      $$
      min\sum_{i=1}^NL(y_i,\sum_{m=1}^M\beta_mb(x_i;\gamma_m))
      $$

    - 缺点：复杂度高。需要更新2M个参数

  - **前向分步算法**具体步骤：

    输入训练数据集T={(x1,y1), (x2,y2), ...,(xN,yN)}；损失函数L(y,f(x))；基函数集{b(x;γ)}

    输出：加法模型f(x)

    - 初始化f0(x)=0

    - 对m=1，2，...，M

      - 极小化损失函数
        $$
        (\beta_m,\gamma_m)=argmin\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+\beta{b(x_i;\gamma)})
        $$
        得到两个参数

      - 更新
        $$
        f_m(x)=f_{m-1}(x)+\beta_mb(x;\gamma_m)
        $$

    - 得到加法模型
      $$
      f(x)=f_M(x)=\sum_{m=1}^M\beta_m{b(x;\gamma_m)}
      $$

### 11.3 算法原理

- 优化问题：二分类

- 模型：加法模型
  $$
  f(x)=\sum_{m=1}^M\alpha_mG_m(x)
  $$

- 最终分类器
  $$
  G(x)=sign[f(x)]
  $$
  
- 损失函数：指数损失函数

  - 二分类问题使用指数损失函数
    $$
    L(y,f(x))=exp[-yf(x)]
    $$
    当G(x)分类正确时，与y同号，L(y,f(x))<=1，当G(x)分类错误时，与y异号，L(y,f(x))>1

    **将损失函数视为训练数据的权值。因为要提高被错误分类的训练样本的权值。**
    $$
    w_{mi}=exp[-y_if_{m-1}(x_i)]
    $$

  - 单个样本损失函数
    $$
    L(y,f_m(x))=exp[-yf_m(x)]=exp[-y\sum_{m=1}^M\alpha_mG_m(x)]=exp[-y(f_{m-1}(x)+\alpha_mG_m(x))]
    $$
    
  - 总体损失函数
    $$
    \sum_{i=1}^Nexp[-y_i(f_{m-1}(x_i)+\alpha_mG_m(x_i))]
    $$
  
- 优化方法：前向分步算法

  - 算法流程

  - 第m轮
    $$
    (\alpha_m,G_m(x))=argmin\sum_{i=1}^Nexp[-y_i(f_{m-1}(x_i)+\alpha{G(x_i)})]
    $$

    - 式子变换
      $$
      (\alpha_m,G_m(x))=argmin(e^{-\alpha}\sum_{y_i=G(x_i)}w_{mi}+e^\alpha\sum_{y_i≠G(x_i)}w_{mi}),w_{mi}=exp[-y_if_{m-1}(x_i)]
      $$
    
  - 求解
    
    - 优化Gm(x)
        $$
        G_m(x)=argmin\sum_{i=1}^Nw_{mi}I(y_i≠G(x_i))
        $$
    
    - 优化αm
        $$
        argmin(e^{-\alpha_m}\sum_{y_i=G(x_i)}w_{mi}+e^{\alpha_m}\sum_{y_i≠G(x_i)}w_{mi})
        $$
    
      $$
      =argmin(e^{-\alpha_m}\sum_{i=1}^Nw_{mi}+(e^{\alpha_m}-e^{-\alpha_m})\sum_{y_i≠G(x_i)}w_{mi})
      $$
    
      求导
      $$
        =-e^{-\alpha_m}\sum_{y_i=G(x_i)}w_{mi}+e^{\alpha_m}\sum_{y_i≠G(x_i)}w_{mi}
      $$
        令导数为0
      $$
        e^{-\alpha_m}\sum_{y_i=G(x_i)}w_{mi}=e^{\alpha_m}\sum_{y_i≠G(x_i)}w_{mi}
      $$
    
      $$
        ln(e^{-\alpha_m}\sum_{y_i=G(x_i)}w_{mi})=ln(e^{\alpha_m}\sum_{y_i≠G(x_i)}w_{mi})
      $$
    
      $$
        -\alpha+ln(\sum_{y_i=G(x_i)}w_{mi})=\alpha+ln(\sum_{y_i≠G(x_i)}w_{mi})
      $$
    
      $$
        2\alpha=ln(\sum_{y_i=G(x_i)}w_{mi})-ln(\sum_{y_i≠G(x_i)}w_{mi})
      $$
    
      $$
        2\alpha=ln(\frac{\sum_{y_i=G(x_i)}w_{mi}}{\sum_{y_i≠G(x_i)}w_{mi}})
      $$
    
      $$
        \alpha=\frac{1}{2}ln(\frac{\sum_{i=1}^Nw_{mi}-\sum_{y_i≠G(x_i)}w_{mi}}{\sum_{y_i≠G(x_i)}w_{mi}})
      $$
    
      $$
        \alpha=\frac{1}{2}ln\frac{1-e_m}{e_m}
      $$
    
    - 前向更新fm(x)
    
    - 更新训练数据权值wmi

## 12、朴素贝叶斯

### 12.1 概率论基础知识

- 条件概率是指事件A在另外一件事件B已发生条件下的发生概率
  $$
  P(A|B)=\frac{P(AB)}{P(B)}
  $$

- 全概率公式
  $$
  P(B)=P(AB+\overline{A}B)=P(AB)+P(\overline{A}B)=P(A)P(B|A)+P(\overline{A})P(B|\overline{A})
  $$

- 贝叶斯公式
  $$
  P(A_k|B)=\frac{P(B|A_k)P(A_k)}{\sum{P(B|A_i)P(A_i)}}=P(B|A_k)P(A_k)
  $$

- 

- 特征条件独立假设就是各条件之间互不影响

### 12.2 朴素贝叶斯模型流程

- 基本方法：在统计数据的基础上，依据条件概率公式，计算当前的特征属于某个分类的概率，选最大的概率分类对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大就属于哪个类别

- 计算流程：

  - x={a1,a2,...,am}为待分类项，每个a为x的一个特征属性

  - 有类别集合c={y1,y2,...,yn}

  - 计算P(y1|x),P(y2|x),...,P(yn|x)

  - P(yk|x)=max{P(y1|x),P(y2|x),...,P(yn|x)}

    - 找到一个已知分类的待分类项集合

    - 得到各类别下各个特征属性的条件概率估计

    - 如果各个特征属性是独立的，根据贝叶斯定理：
      $$
      P(y_i|x)=\frac{P(x|y_i)P(y_i)}{P(x)}
      $$

- 三个阶段

  - 准备阶段：根据具体情况确定特征属性，对每个特征属性进行划分，然后由人工对部分待分类项进行分类，形成训练样本集合
  - 训练阶段：计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计
  - 应用：使用分类器对待分类项进行分类

### 12.3 拉普拉斯平滑

为了解决0概率的问题，在每个分类加1，因训练样本较大，可忽略不计

### 12.4 面试题

- 朴素贝叶斯和LR的区别：

  ​	朴素贝叶斯是生成模型，根据已有样本进行贝叶斯估计学习出先验概率P(Y)和P(X|Y)，进而求出联合概率P(XY)，最后利用贝叶斯定理求解P(Y|X)。朴素贝叶斯基于很强的条件独立假设条件。适用于数据集少。

  ​	而LR是判别模型，根据极大化对数似然函数直接求出条件概率P(Y|X)，LR适用于规模大的数据集。

- 朴素在哪：朴素贝叶斯做了很强的**条件独立假设**
- 在估计条件概率P(X|Y)时出现概率0怎么办：拉普拉斯平滑
- 优缺点：
  - 优点：对小规模的数据训练表现很好，适合多分类，适合增量式训练
  - 缺点：对输入数据的表达方式很敏感

# 二、验证方式

## 1、过拟合

过拟合是指模型在训练集上的效果很好，在测试集的预测效果很差。

产生原因：

- 数据本身有噪声
- 训练数据不足
- 训练模型过于复杂

如何防止过拟合：

- 早停策略：在训练数据集时加入训练集和验证集，当验证集误差开始上升且训练集误差不断下降时提前结束训练。
- 数据集扩增：增加原有数据，数据增强。
- 正则化：L1正则化、L2正则化
- dropout：**每次训练时舍弃一些节点，增强模型泛化能力**
- 减小模型复杂度
- 集成方法：**随机森林**（减少的是方差）

## 2、欠拟合

欠拟合是由于模型复杂度过于简单或者数据过少，对模型数据的拟合程度不高，因此在训练集上的训练效果不好。

如何防止欠拟合：

- 增加样本数量
- 增加样本特征个数
- 减少正则化参数
- 集成学习：Boosting（减少的是偏差）

## 3、训练集、验证集、测试集的作用

训练集：用于模型**训练拟合**的数据样本

验证集：是模型训练过程中单独留出来的样本集，它可以用于调整模型的超参数和用于对模型的拟合能力进行初步评估。通常用来在模型迭代训练时，用以验证当前模型泛化能力，以决定是否停止继续训练。

测试集：**用来评估最终模型的泛化能力**。但不能作为调参、选择特征等算法相关的选择依据。

## 4、交叉验证

### 	4.1 目的

尝试利用不同训练集/验证集的划分来对模型做多组不同的训练/验证，**来应对单独测试结果过于片面以及训练数据不足的问题。**

### 	4.2 方法

- 留出法：简单地将原始数据集划分为**训练集、验证集、测试集三个部分**
- k折交叉验证：将原始数据集划分为k个子集，将其中一个子集作为验证集，其余k-1个子集作为训练集，如此训练和验证一轮称为一次交叉验证。交叉验证重复k次，每个子集都做一次验证集，得到k个模型，**加权平均k个模型的结果作为评估整体模型的依据。**
- 留一法：只留一个样本作为数据的测试集，其余作为训练集（**只适用于较小的数据集**）
- Bootstrap方法：重采样方法

### 	4.3 k折交叉验证算法

- 随机将训练数据等分为k份
- 对于每一个模型，算法执行k次，每次选择一份作为验证集，而其他作为训练集来训练模型M，把训练得到的模型在验证集上测试，得到一个误差e。对k次得到的误差求平均，可以得到模型的泛化误差。
- **选择泛化误差最小的模型作为最终模型，并在整个训练集上再次训练，得到最终的模型。**

### 	4.4 k折交叉验证中如何选择k

k越大不一定效果越好，越大的k训练时间越长。

在选择k时，需要考虑**最小化数据集之间的方差**，对于2分类问题，采用2折交叉验证，即将原始数据对半份，若此时训练集中都是A类别，验证集中都是B类别，则交叉验证效果会很差。

# 三、分类

## 1、 模型常见的评估指标

|          |      | 实际结果 |      |
| -------- | ---- | -------- | ---- |
|          |      | 1        | 0    |
| 预测结果 | 1    | TP       | FP   |
|          | 0    | FN       | TN   |

TP：True Positive 真阳

FP：False Positive 假阳 

**准确率 =（TP+TN）/ 总样本数=预测正确的结果占总样本的百分比**

### 1.1 Precision查准率

精准率（Precision）=TP/(TP+FP)=所有被**预测为正的样本**中实际为正的样本的概率

### 1.2 Recall查全率

**召回率（Recall）=TP/(TP+FN)**=**在实际为正的样本中被预测为正样本的概率**

查准率和查全率是一对矛盾的度量，一般而言，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。

### 1.3 P-R曲线

![IMG_256](G:\面试\机器学习\P-R曲线.png)

横轴为召回率，纵轴为精准率。对于排序模型来说，其P-R曲线上的一个点代表"在某一阈值下模型将大于该阈值的结果判定为正样本，将小于该阈值的结果判定为负样本时，排序结果对应的召回率和精准率"。

整条P-R曲线是通过从高到低移动正样本阈值生成的。随着召回率的升高，精确率以不同的速度变化，只用一个点的精确率和召回率是不能全面衡量模型性能的。P-R曲线底下的面积越大表示模型的排序性能越好。

**引入平衡点BEP来度量，表示精准率=召回率时的取值，值越大表明分类器性能越好。**

### 1.4 F1分数

$$
F_1=2\frac{precision·recall}{precision+recall}
$$

调和平均：
$$
\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}
$$
精准率和召回率的权衡：只有在召回率Recall和精准率Precision都高的情况下，F1 score才会很高，比BEP更常用。

### 1.5 ROC和AUC曲线

![IMG_256](G:\面试\机器学习\ROC曲线.jpg)

ROC曲线：横轴为FPR，纵轴为TPR。是显示Classification模型真正例率和假正例率之间折中的一种图形化方法。

**真正例率TPR=TP/(TP+FN)**，**所有确实为真的样本中，被判断为真的样本概率**。即召回率。

**假正例率FPR=FP/(FP+TN)**。**所有确实为假的样本中，被误判为真的样本概率**。

TPR越高，同时FPR越低（即ROC曲线越陡）
$$
AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)·(y_i+y_{i+1})
$$
AUC：

- **ROC曲线下的面积，AUC的取值范围在0.5和1之间**
- 给定正样本M个，负样本N个，以及他们的预测概率，那么AUC的含义就是穷举所有的正负样本对，如果正样本的预测概率大于负样本的预测概率，那么+1；如果正样本的预测概率等于负样本的预测概率，那么+0.5；否则，+0。最后把统计处理的个数除以M*N就得到我们的AUC。

$$
AUC=\frac{\sum_{i\in{positiveClass}}rank_i-\frac{M(1+M)}{2}}{MN}
$$
$$
AUC=\frac{\sum{I(P_{正样本},P_{负样本})}}{M*N}
$$

衡量二分类模型优劣的一种评价指标，**表示随机给定一个正样本和一个负样本，用一个分类器进行预测，该正样本的得分比该负样本的得分要大的概率。**

### 1.6 如何解释AUC、ROC分数

表示预测准确性。AUC值越高，预测准确率越高，反之，预测准确率越低。

**AUC如果小于0.5，说明预测准确性比随机性猜测还差，实际情况中不应该出现这种情况**，可能是设置的状态变量标准有误。

AUC的计算方法**同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价**

- 如果改变正负样本的采样比例，**AUC值为何具有鲁棒性**

  ![img](G:\面试\机器学习\TPR-FPR.jpg)

  **FPR和TPR的分母分别是负样本个数和真样本个数，当样本空间确定时，FPR和TPR的分母都是定值。**

  假设我们有M个样本，根据模型预估得到了M个预估分，**根据预估分从高到低排序，再设定一个阈值指针，阈值指针从最高分的样本到最低分移动，每一次移动后，以该阈值指针为分割点，大于等于该阈值指针指向样本预估分的样本，为预测正例，小于的为预测负例，并计算对应的FPR和TPR对**，这样阈值指针移动完就得到了M对，就可以画ROC曲线了。

  - 假设有N个正样本和N个负样本

    如果将正样本和负样本采样比例变为1:2，正例数随机减少一半，变为N/2，样本总数为3N/2

    首先，FPR的分母还是N，分子FP表示将负例预估为正例的个数，对于阈值指针指向的任何一个样本，预估分大于等于该样本对应的预估分的负样本，原来有多少，现在还是有多少。所以FPR没有任何变化。
  
    其次，FPR分母变为N/2，TP表示把正样本预估成正例的个数，因为我们随机删去一般正样本，所以大于该预估分的正样本数量也会随机删掉一半，所以TP也近似减少一般，所以TPR也近似没有变化。

```python
def auc_calculate(label, pre):
    pos = [i for i in range(len(label)) if label[i] == 1]
    neg = [i for i in range(len(label)) if label[i] == 0]
    auc = 0
    for i in pos:
        for j in neg:
            if pre[i] > pre[j]:
                auc += 1
            elif pre[i] == pre[j]:
                auc += 0.5
    return auc / (len(pos)*len(neg))
```

### 1.7 AUC线上线下不一致怎么办

- 模型是否一致：主要包括离线模型格式转换、serving部署，线上模型加载、预估等接口是否有问题
- 特征是否一致
  - 校验线上线下特征处理逻辑是否一致
  - 与线上真实预估环境相比，离线环境更容易获取到特征，当离线使用线上获取不到的特征时，就会导致离线效果虚高的假象

# 四、正则化

正则化定义：在损失函数后面加上一个**正则化项（惩罚项）**，其实就是常说的结构风险最小化策略，即损失函数加上正则化。一般模型越复杂，正则化值越大。

正则化的目的：限制参数过多或者过大，避免模型更加复杂。

正则化项是对模型中某些参数进行约束，正则化的一般形式：
$$
min\frac{1}{n}\sum_{i=1}^{n}{L(y_i,f(x_i))}+\lambda J(f)
$$
第一项是损失函数，第二项是正则化项。加上惩罚项后损失函数的值会变大，为了使得损失函数值最小，惩罚项的值就要尽可能地小，即模型的**参数量**就要尽可能地小。

- L0范数：计算向量中非0元素的个数
- L1范数和L0范数的目的是使参数稀疏化
- L1范数比L0范数容易优化求解（L1范数是L0范数的最优凸近似）

|          | L1正则                                                 | L2正则                   |
| -------- | ------------------------------------------------------ | ------------------------ |
| 定义     | 向量中各元素绝对值之和                                 | 向量中各元素平方和的开方 |
| 联系     | 降低损失函数                                           | 降低损失函数             |
| 函数分布 | **拉普拉斯分布**                                       | **高斯分布**（正态分布） |
| 作用     | 产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择 | 防止模型过拟合           |

## 4.1 L1正则化（Lasso 稀疏规则算子）

L1范数是指向量中各个元素绝对值之和。**为什么L1范数会使权值稀疏：**

- 它是L0范数的最优凸优化
- 任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个求和的形式，那么这个规则化算子就可以实现稀疏
- 相比于L2正则化，L1正则化会产生更稀疏的解。**此处稀疏性指的是最优值中的一些参数为0**。和L2正则化相比，L1正则化的稀疏性具有本质的不同。

**为什么要稀疏？参数稀疏有什么好处？**

- 特征选择：

  稀疏规则化关键原因是在于它能实现特征的自动选择，一般来说，输入的大部分元素都是和最终的输出没有关系或者不提供任何信息的，在最优化目标函数的时候考虑输入这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确输出的预测。**稀疏规则化算子的引入就是为了完成特征自动选择的使命，它会学习去掉这些没有信息的特征，把这些特征对应的权重置为0**

- 可解释性：

  另一个好处是**参数变少可以使得整个模型获得更好的可解释性。**

L1在0处不可导如何处理？

- 坐标轴下降法：沿着坐标轴下降的方向
  - 假设有m个特征，坐标轴下降法进参数更新的时候，先固定m-1个值，然后再求另外一个的局部最优解，从而避免损失函数不可导的问题。

- 近端梯度下降
- 交替方向乘子法

## 4.2 L2正则化 （Ridge）

L2范数在回归里面，它的回归叫"岭回归"（Ridge Regression），也叫它权值衰减（weight decay）

L2范数正则——权值衰减带来的影响

- 只有在显著减小目标函数方向上的参数会保留得相对完好。在无助于目标函数减小的方向上改变参数不会显著增加梯度。这种不重要方向对应的分量会在训练过程中因正则化而衰减掉。
- 权重衰减的效果是沿着由H的特征向量所定义的轴缩放w

L2范数的优点

- 学习理论角度：L2范数可以防止过拟合，提升模型的泛化能力
- 优化计算角度：从优化或者数值计算角度来说，L2范数有助于处理条件数不好的情况下矩阵求逆很困难的问题

为什么L2范数可以防止过拟合？模型越简单？

- L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它只会接近于0。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。
- 越复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。

## 4.3 L1和L2的差别

**函数角度的解释：**

假设不加正则项的损失函数为L(x)

- 函数极值的判断定理如下

  - 定理1：当该点导数存在，且该导数等于零时，则该点为极值点
  - 定理2：当该点导数不存在，左导数和右导数的符号相异时，则该点为极值点

- 加L2正则项：

  f(x)=L(x)+Cx^2,C>0 对f(x)求导，得f'(x)=L'(x)+2Cx。当x=0时，有f'(0)=L'(0)+0=L'(0)。即加了L2正则化的损失函数。f(x)在x=0点处的导数是等于不加L2正则化的原损失函数在x=0的导数才为0。

- 加L1正则项：

  f(x)=L(x)+C|x|,C>0 该函数在x=0处是不可导的，其在x=0处的左右导数为：
  $$
  f'(0^-)=L'(0)-C
  $$

  $$
  f'(0^+)=L'(0)+C
  $$

  根据定理二，令
  $$
  f'(0^-)*f'(0^+)<0
  $$
  即当(L'(0)-C)*(L'(0)+C)<0，该点为f(x)的极值点，解得C>|L'(0)|，即**只要满足参数C大于原损失函数在x=0处的导数的绝对值，那么f(x)就能在x=0处取得极值。**

  综上所述，**满足L1在x=0处取得极值的条件比L2简单得多，所以L1相比L2更容易获得稀疏解。**

**用图像来呈现：**

假设损失函数L与某个参数x的关系如图所示：

![img](G:\面试\机器学习\损失函数.jpg)

则最优的x在绿点处，x非零。现在施加L2正则化，新的损失函数 (L+Cx^2) 如图蓝线所示

![img](G:\面试\机器学习\损失函数+L2.jpg)

最优的x在黄点处，x的绝对值减小了，但依然非零

而施加L1正则化，则新的损失函数(L+C|x|)如图粉线所示

![img](G:\面试\机器学习\损失函数+L2+L1.jpg)

最优的x就变成了0。这里利用的就是绝对值函数的尖峰。

**两种正则化能不能把最优的x变为0，取决于原先的损失函数在0处的导数。如果本来导数不为0，那么施加L2正则化后导数依然不为0。最优的x也不会变为0。而施加L1正则化时，只要正则化的系数大于原损失函数在0点处的导数的绝对值，x=0就会变成一个极值点。**

**几何上的直观解析：**

- 下降速度：L1和L2都是正则化的方式，我们将权值参数以L1或者L2的方式放到目标函数里面，然后模型就会尝试去最小化这些权值参数。这个最小化就像一个下坡的过程，L1和L2的差别就在于这个坡不同。如下图：L1就是按绝对值函数的坡下降的，而L2是按二次函数的坡下降，所以实际上在0附近，L1的下降速度比L2的下降速度要快，所以会非常快降到0。

  ![img](G:\面试\机器学习\L1和L2下降曲线.png)

- 模型空间的限制

  对于L1和L2正则化的损失函数，我们可以写成以下形式

  ![img](G:\面试\机器学习\L1和L2损失函数.png)

  为了便于可视化，我们考虑两维的情况，在（w1，w2）平面上可以画出目标函数的等高线，而约束条件则称为平面上半径为C的一个norm ball，等高线与norm ball首次相交的地方就是最优解。

  ![img](G:\面试\机器学习\L1和L2等高线.png)

  彩色线就是优化过程中遇到的等高线，一圈代表一个目标函数值，圆心就是样本观测值，半径就是误差值，受限条件就是黑色边界（就是正则化那部分），二者相交处，才是最优参数。

  可以看到，L1-ball与L2-ball的不同就在于L1在和每个坐标轴相交的地方都有角出现，**而目标函数除非位置摆的很好，大部分情况都会在角的地方相交。注意到在角的位置就会产生稀疏性。**

  相比之下，L2-ball就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1正则化能产生稀疏性，而L2正则化不行的原因了。

# 五、特征工程

特征工程分三步：1、数据预处理；2、特征选取；3、特征提取

## 1、数据预处理

### 1.1 为什么要对数据做归一化

- 归一化后加快的梯度下降对最优解的速度
- 归一化有可能提高精度

## 1.2 归一化的种类

- 线性归一化：利用max和min进行归一化，如果max和min不稳定，则常用经验值来替代max和min
  $$
  x’=\frac{x-min(x)}{max(x)-min(x)}
  $$

- 标准差归一化：利用所有样本的均值和方差将样本归一化为正态分布
  $$
  x^*=\frac{x-\mu}{\sigma}
  $$

- 非线性归一化：比如指数、对数、三角函数等

## 1.3 归一化和标准化的区别

- 标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到统一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为"单位向量"。
- 归一化的目的是方便比较，可以加快网络的收敛速度；标准化是将数据利用z-score（均值、方差）的方法转化为符合特定分布的数据，方便进行下一步处理，不为比较

## 1.4 需要归一化的算法有哪些？这些模型需要归一化的主要原因

线性回归、逻辑回归、KNN、SVM、神经网络

主要是因为特征值相差很大时，运用梯度下降，损失等高线是椭圆形，需要进行多次迭代才能达到最优点，如果进行归一化了，那么等高线就是圆形的，促使SGD往原点迭代，从而导致需要迭代次数较少

## 1.5 树形结构不需要归一化的原因

**因为它们不关心变量的值，而是关心变量分布和变量之间的条件概率，如决策树，随机森林；对于树形结构，树模型的构造是通过寻找最优分裂点构成的，样本点的数值缩放不影响分裂点的位置，对树模型的结构不造成影响。**

而且树模型不能进行梯度下降，因为**树模型是阶跃的，阶跃是不可导的，并且求导没意义，也不需要归一化。**

## 2、特征选择

特征选择在于选取对训练数据具有分类能力的特征，可以提高决策树学习的效率。通常特征选择的准则是**信息增益或信息增益率。**

特征选择的划分依据：这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就应该选择这个特征。（将数据集划分为纯度更高，不确定性更小的子集的过程）

### 2.1 什么是特征选择？

指从已有的M个特征中选择N个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程。

原因：

- **减少特征数量、降维，使模型泛化能力更强，减少过拟合**
- 增强对特征和特征值之间的理解

目标：**选择离散程度高且与目标的相关性强的特征**

## 2.2 有哪些特征选择技术

- 过滤法：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，从而选择特征
- 包装法：根据目标函数（通常是预测效果评分），每次选择若干特征或者排除若干特征，常用方法主要是递归特征消除法。
- 嵌入法：先使用ML的算法和模型进行训练，得到各个特征的权重系数，根据系数从大到小选择特征；常用方法主要是基于惩罚项的特征选择法

![IMG_256](G:\面试\机器学习\特征选择.jpg)

## 3、特征提取

常见的降维方法除了基于L1惩罚项的模型以外，还有PCA和LDA。本质是要将原始的样本映射到维度更低的样本空间中。

但是它们的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性，而LDA是为了让映射后的样本有最好的分类性能

### 3.1 特征选择与特征提取的区别

都是降维的方法。

特征选择：不改变变量的含义，仅仅只是做出筛选，留下对目标影响较大的变量

特征提取：通过映射的方法，将高维的特征向量变换为低维特征向量

### 3.2 为什么要处理类别特征？怎么处理？

除了决策树等少量模型能直接处理字符串形式的输入，对于LR、SVM等模型来说，类别特征必须经过处理转化为数值特征才能正常工作。

方法：

- 序号编码
- **独热编码**
- 二进制编码

### 3.3 什么是组合特征

为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高级特征。例如，特征a有m个取值，特征b有n个取值，将二者组合就有m*n个组合情况。

### 3.4 如何解决数据不平衡问题

主要是由于**数据分布不平衡**造成的。

- 通过**过抽样和欠抽样**解决样本不均衡

  抽样是解决样本分布不均衡相对简单且常用的方法，包括过抽样和欠抽样两种

  - 过抽样（也叫上采样、over-sampling）方法通过增加分类中少数类样本的数量来实现样本均衡，最直接的方法是简单复制少数类样本形成多条记录，**这种方法的缺点是如果样本特征少而可能导致过拟合的问题**；经过改进的过抽样方法通过在少数类中加入随机噪声、干扰数据或通过一定规则产生新的合成样本，例如SMOTE算法
  - 欠抽样（也叫下采样、under-sampling）方法通过减少分类中多数类样本的样本数量来实现样本均衡，**最直接的方法是随机地去掉一些多数类样本来减小多数类的规模，缺点是会丢失多数类样本中的一些重要信息。**

  总体上，过抽样和欠抽样更适合大数据分布不均衡的情况，尤其是第一种方法应用更加广泛

- 通过正负样本的惩罚权重解决样本不均衡

  通过正负样本的惩罚权重解决样本不均衡的问题的思想是算法实现过程中，对于分类中不同样本数量的类别分别赋予不同的权重（**一般思路分类中的小样本量类别权重高，大样本量类别权重低**），然后进行计算和建模

- 通过组合/集成方法解决样本不均衡

  组合/集成方法指的是在每次生成训练集时**使用所有分类中的小样本量**，同时从分类中的**大样本量中随机抽取数据来与小样本量合并构成训练集**，这样反复多次会得到很多训练集和训练模型。最后在应用的时候，使用组合方法产生分类预测结果。

- 通过特征选择解决样本不均衡

  一般情况下，样本不均衡也会导致特征分布不均衡，但如果小类别样本量具有一定的规模，那么意味着其特征值的分布较均匀，可通过**选择具有显著性的特征**配合参与解决样本不均衡问题，也能在一定程度上提高模型效果。

### 3.5 数据中有噪声如何处理

检查噪声常见的方法：

- 通过寻找数据集中与其他观测值及均值差距最大的点作为异常
- 聚类方法检测：将类似的取值组织成'群'或者'簇'，落在'簇'集合之外的值被视为离群点。

如何去掉数据中的噪声

- 分箱：
  - 等深分箱法
  - 等宽分箱法
  - 最小熵法
  - 用户自定义区间法
- 平滑方法
  - 按平均值平滑：对同一箱值中的数据求平均，用平均值替代该箱子中的所有数据
  - 按边界值平滑：用距离较小的边界值替代箱中每一数据
  - 按中值平滑：取箱子的中值，用来替代箱子的所有数据

## 4、经验风险和结构风险

- 经验风险：当样本容量足够大的时候，经验风险最小化能保证有很好的学习效果，在现实中被广泛采用。例如，极大似然估计（MLE）就是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等于极大似然估计。
- 结构风险：但是当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生过拟合现象。而结构风险最小化（SRM）是为了防止过拟合而提出的策略。结构风险最小化等价于正则化。结构风险在经验风险的基础上加上表示模型复杂度的正则化项，降低模型复杂度，提升模型的可预测性。

# 六、集成学习

定义：**通过结合多个学习器**（例如同种算法但是参数不同，或者不同算法），**一般会获得比任意单个学习器都要好的性能**，尤其是在**这些学习器都是"弱学习器"的时候提升效果会很明显。**

## 1、Boosting提升法

可以用于**回归和分类问题**，它每一步产生一个弱预测模型（如决策树），并加权累加到总模型中；如果每一步的弱预测模型生成都是依据损失函数的梯度方向，则称之为梯度提升。

梯度提升算法首先给定一个目标损失函数，它的定义域是所有可行的弱函数集合，提升算法通过迭代地选择一个负梯度方向上的基函数来逐渐逼近局部最小值。

提升的理论意义：如果一个问题存在弱分类器，则可以通过提升的方法得到强分类器

**特点：**个体学习器之间存在强依赖关系，必须串行生成的序列化方法

工作机制：

- 提高那些在前一轮被弱分类器分错的样本的权值，减小那些在前一轮被弱分类器分对的样本的权值，使得误分的样本在后续受到更多的关注
- 加法模型将弱分类器线性组合

### 1.1 GBDT梯度提升

GBDT是迭代，但GBDT每一次的计算都是为了减少上一次的残差，进而在残差减少（负梯度）的方向上建立一个新的模型，其弱学习器限定了只能使用CART回归树模型。

![IMG_256](G:\面试\机器学习\GBDT.jpg)

#### 1.1.1 GBDT训练过程如何选择特征

GBDT使用的基学习器是**CART树，CART树是二叉树**，每次使用yes or no进行特征选择，数值连续特征使用的最小均方误差，离散值使用的Gini指数。**在每次划分特征的时候会遍历所有可能的划分点找到所有的特征分裂点，这就是用GBDT会比RF慢的主要原因之一。**

#### 1.1.2 GBDT如何防止过拟合？由于GBDT是前向加法模型，前面的树往往起到决定性的作用，如何改进这个问题？

一般使用缩减因子对每棵树进行降权，可以使用带有**dropout的GBDT算法，CART树，随机丢弃生成的决策树，然后再从剩下的决策树集中迭代优化提升树。**

GBDT和Boosting区别较大，**它的每一次计算都是为了减少上一次的残差**，而为了消除残差，可以在残差减小的梯度方向上建立模型。

在GradientBoost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法。

#### 1.1.3 GBDT对标量特征要不要one-hot编码

从效果的角度来讲，使用category特征和one-hot是等价的，所不同的是category特征的feature空间更小。catefory特征可以直接输入，不需要one-hot编码，准确度差不多，速度更快。

#### 1.1.4 为什么GBDT用负梯度当作残差

- 负梯度的方向可证，模型优化下去一定会收敛
- 对于一些损失函数来说最大的残差方向，并不是梯度下降最好的方向，因为损失函数最小与残差最小两者目标不统一

### 1.2 AdaBoost 自适应提升

定义：是一种提升方法，将多个弱分类器，组合成强分类器。

AdaBoost既可以用作分类，也可以用作回归。

算法实现：

- 提高上一轮被错误分类的样本的权值，降低被正确分类的样本的权值
- **线性加权求和**。误差率小的基学习器拥有较大的权值，误差率大的基学习器拥有较小的权值

| 优点                                       | 缺点                   |
| ------------------------------------------ | ---------------------- |
| 精度很高的分类器                           | 可以用于二分类或多分类 |
| 提供的是框架，可以使用各种方法构建弱分类器 | 特征选择               |
| 简单，不需要做特征筛选                     | 分类任务的baseline     |
| 不用担心过拟合                             |                        |

#### 1.2.1 为什么AdaBoost 方式能够提高整体模型的学习精度

根据前向分布加法模型，AdaBoost算法每一次都会降低整体的误差，**虽然每个模型误差会有波动，但是整体的误差却在降低，整体模型复杂度在提高。**

#### 1.2.2 使用m个基学习器和加权平均使用m个学习器之间有什么不同

AdaBoost的m个基学习器是有顺序关系的，**第k个基学习器根据前k-1个学习器得到的误差更新数据分布，再进行学习，每一次的数据分布都不同，是使用同一个学习器在不同的数据分布上进行学习。**

加权平均的m个学习器是可以并行处理的，在同一个数据分布上，学习得到m个不同的学习器进行加权

#### 1.2.3 AdaBoost 的迭代次数（基学习器的个数）如何控制？

一般使用早停法进行控制迭代次数。

#### 1.2.4 AdaBoost 算法中基学习器是否很重要，应该如何选择基学习器

一般认为决策树表现良好，其实可以根据数据的分布选择对应的分类器，比如选择简单的LR，或者对于回归问题选择线性回归。

### 1.3 XGBoost 极端梯度提升

基于Boosting增强策略的加法模型，训练的时候采用前向分步算法进行贪婪学习，每次迭代都学习一颗CART决策树来拟合之前t-1棵树的预测结果与训练样本真实值的残差。

XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则化、支持并行核默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。

![IMG_256](G:\面试\机器学习\XGBoost.png)

#### 1.3.1 XGBoost使用泰勒二阶展开的原因

- 精准性：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更精准地逼近真实的损失函数
- 可扩展性：损失函数支持自定义，只需要新的损失函数二阶可导

#### 1.3.2 XGBoost可以并行训练的原因

XGBoost的并行，并不是说每棵树可以并行训练，XGBoost本质上仍然采用Boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。

XGBoost的并行，指的是**特征维度的并行**：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。

#### 1.3.3 XGBoost为什么快

| 分块并行          | 训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点 |
| ----------------- | ------------------------------------------------------------ |
| 候选分位点        | 每个特征采用常数个分位点作为候选分割点                       |
| CPU cache命中优化 | 使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中 |
| Block处理优化     | Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐 |

#### 1.3.4 XGBoost防止过拟合的方法

- 目标函数添加正则项：叶子节点个数+叶子节点权重的L2正则化
- 列抽样：训练的时候只用一部分特征
- 子采样：每轮计算可以不适用全部样本
- shrinkage：可以叫学习率

#### 1.3.5 XGBoost如何处理缺失值

在特征k上寻找最佳分割点时，不会对该列特征None的样本进行遍历，而只对该列特征值为非空的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找分割点的时间开销。

在逻辑实现上，为了保证完备性，会将该特征值为空的样本分别分配到左叶子节点和右叶子节点，两种情形都计算一遍后，选择分裂后增益最大的方向，作为预测时特征值缺失样本的默认分支方向。

如果在训练中没有缺失值而在预测中出现缺失值，那么会自动将缺失值的划分方向放到右子节点。

#### 1.3.6 为什么XGBoost相比某些模型对缺失值不敏感？

对存在缺失值的特征，一般的解决方法是：

- 离散型变量：用出现次数最多的特征值填充
- 连续型变量：用中位数或均值填充

一棵树中每个节点在分裂时，寻找的是某个特征的最佳分裂点（特征值），完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本的特征值缺失，对寻找最佳分割点的影响不大，对于有缺失值的数据在经过缺失处理后：

- 当数据量很小时：优先使用朴素贝叶斯
- 数据量适中或者较大：用树模型，优先XGBoost
- 数据量较大：用神经网络
- 避免使用距离度量相关的模型，如KNN和SVM

#### 1.3.7 XGBoost如何处理不平衡数据

- 如果你在意AUC，采用AUC来评估模型的性能，那你可以平衡正负样本的权重
- 如果你在意概率（预测得分的合理性），你不能重新平衡数据集（会破坏数据的真实分布），应该设置max_delta_step为一个有限数字来帮助收敛

#### 1.3.8 比较LR和GBDT，说说什么情境下LR不如GBDT

- LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程。


- GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合。
- 在高维稀疏特征的场景下，LR的效果一般会比GBDT好。

#### 1.3.9 XGBoost在什么地方做的剪枝？如何进行剪枝？

- 目标函数，使用叶子的数目和l2模的平方，控制模型的复杂度
- 在分裂节点的计算增益中，定义了一个阈值，当增益大于阈值才分裂，先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的节点，进行剪枝。

#### 1.3.10 XGBoost如何选择最佳分裂点

XGBoost在训练前预先将特征按照特征值进行了排序，并存储为Block结构，以后在结点分裂时可以重复使用该结构。

因此，可以采用特征并行的方法利用多线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，最终选择增益最大的那个特征的特征值作为最佳分裂点。

如果在计算每个特征的最佳分割点时，对每个样本进行遍历，计算复杂度会很大，这种全局扫描的方法并不适用大数据的场景。XGBoost还提供了一种直方图近似算法，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。

#### 1.3.11 XGBoost的Scalable性如何体现？

- 基分类器：弱分类器可以支持CART决策树，也可以支持LR和linear
- 目标函数：支持自定义损失函数，只需要其一阶、二阶可导。
- 学习方法：Block结构支持并行化。

#### 1.3.12 XGBoost模型如果过拟合了怎么办

- 直接控制模型的复杂度：最大深度、gamma
- 增加随机性：使得模型在训练时对于噪声不敏感

#### 1.3.13 XGBoost如何寻找最优特征？是有放回还是无放回

XGBoost利用梯度优化模型算法，样本是不放回的。但XGBoost支持子采样，也就是每轮计算可以不使用全部样本

#### 1.3.14 XGBoost如何分布式？特征分布式和数据分布式？各有什么问题？

XGBoost在训练之前，预先对数据按列进行排序，然后保存block结构

- 特征分布式（特征间并行）：由于将数据按列存储，可以同时访问所有列，那么可以对所有属性同时执行切分点寻找算法，从而并行化切分点寻找
- 数据分布式（特征内并行）：可以用多个block分别存储不同的样本集，多个block可以并行计算

问题：

- 不能从本质上减少计算量
- 通讯代价高

#### 1.3.15 为什么XGBoost的近似算法比LightGBM慢很多呢？

XGBoost在每一层都动态构建直方图，因为XGBoost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图，所以每一层都要重新构建直方图，而LightGBM中每个特征都有一个直方图，所以构建一次直方图就够了。

#### 1.3.16 XGBoost如何评价特征的重要性

- weight：该特征在所有树中被用作分割样本的特征的总次数
- gain：该特征在其出现过的所有树中产生的平均增益
- cover：该特征在其出现过的所有树中的平均覆盖范围

### 1.4 LightGBM

### 1.5 CatBoost

## 2、Bagging（套袋法）和Boosting（提升法）

- 个体学习器之间不存在强依赖关系、可同时生成的并行化方法。

  - **工作机制：**

  - 从原始样本集中抽取出**k个训练集**
    - 每轮从原始样本集中使用Bootstraping法（即自助法，是一种有放回的抽样方法，可能抽到重复的样本）抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中），共进行k轮抽取，得到k个训练集（k个训练集相互独立）
    - 随机森林中，还会随机抽取一定数量的特征

  - k个训练集分别训练，共得到k个模型

  - 将上步得到的k个模型，通过一定方式组合起来
    - 分类问题：将上步得到的k个模型采用投票的方式得到分类结果
    - 回归问题：计算上述模型的均值作为最后的结果


- Boosting是一种将弱分类器转化为强分类器的方法统称，而AdaBoost是其中的一种
  - 算法：
  - 对于训练集中的每个样本建立权重wi，表示对每个样本的权重，其关键在对于被错误分类的样本权重会在下一轮的分类中获得更大的权重（错误分类的样本权重增加）
  - 同时加大分类误差概率小的弱分类器的权值，使其在表决中起到更大的作用，减小分类误差率较大的弱分类器的权值，使其在表决中起到较小的作用。每一次迭代都得到一个弱分类器，需要使用某种策略将其组合，作为最终模型

### 2.1 随机森林

## 3、Bagging vs Boosting

- 取样方式不同：Bagging采用均匀采样，Boosting根据错误率采样，因此Boosting的分类精度要优于Bagging
- Bagging的训练集的选择是随机的，各轮训练集之间相互独立，而Boosting的各轮训练集的选择与前面各轮的学习结果有关
- Bagging的预测函数没有权重，而Boosting是有权重的
- Bagging的各个预测函数可以并行生成，而Boosting的各个预测函数只能顺序生成

## 4、随机森林 vs GBDT

- 相同点：
  - 都是由多棵树组成的，最终结果都是由多棵树共同决定的
- 不同点：
  - 集成学习：RF属于bagging思想，而GBDT属于boosting思想
  - 偏差-方差权衡：RF不断地降低模型方差，而GBDT不断降低模型偏差
  - 训练样本：RF每次迭代的样本是从全部训练集中有返回抽样形成的，而GBDT每次使用全部样本
  - 并行性：RF的树可以并行生成，而GBDT的树需要顺序生成
  - 最终结果：RF最终是多棵树进行多数表决（回归问题取平均值），而GBDT是加权融合
  - 数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感
  - 泛化能力：RF不易过拟合，GBDT容易过拟合

## 5、AdaBoost vs GBDT

最主要的区别在于两者如何识别模型的问题。

和AdaBoost一样，GBDT也是重复选择一个表现一般的模型并且每次基于先前模型的表现进行调整。**不同的是，AdaBoost是通过提升错分数据点的权重来定位模型的不足而GBDT是通过算梯度来定位模型的不足。**

因此相比AdaBoost，GBDT可以使用更多种类的目标函数，而当目标函数是均方误差时，计算损失函数的负梯度值在当前模型的值即为残差。

## 6、XGBoost vs GBDT

- 基分类器：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化的Logistic回归（分类问题）或者线性回归（回归问题）
- 导数信息：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，而且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导
- 正则项：XGBoost的目标函数加了正则化，相当于预剪枝，使得学习出来的模型更加不容易过拟合
- 列抽样：XGBoost支持列采样，与随机森林相似，用于防止过拟合
- 缺失值处理：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支
- 并行化：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。

## 7、XGBoost vs LightGBM

- 树生长策略：XGBoost采用level-wise的分裂策略，LGB采用leaf-wise的分裂策略。XGBoost对每一层结点做无差别分裂，但是可能有些结点增益非常小，对结果影响不大，带来不必要的开销。Leaf-wise是在所有叶子结点中选取分裂收益最大的结点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制。
- 分割点查找算法：XGBoost使用特征预排序算法，LGB使用基于直方图的切分点算法，其优势如下
  - 减少内存占用，比如离散为256个bin时，只需要8位整型就可以保存一个样本被映射为哪个bin，对比预排序的exact greedy算法来说，可以节省7/8的空间
  - 计算效率提高。预排序需要对每个特征都遍历一遍数据，并计算增益。而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可
  - LGB还可以使用直方图做差加速，一个结点的直方图可以通过父节点的直方图减去兄弟结点的直方图得到，从而加速计算
  - 实际上XGBoost的近似直方图也类似于LGB的直方图算法，为什么还是慢？
    - XGBoost在每一层都动态构建直方图，因为XGBoost的直方图算法不是针对某个特定的feature，而是所有feature共享一个直方图，所以每一层都要重新构建直方图，而LGB中对每个特征都有一个直方图，所以构建一次直方图就可以了
- 支持离散变量：无法输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而LGB可以直接处理类别型变量
- 缓存命中率：XGBoost使用Block结构的一个缺点就是取梯度的时候，是通过索引获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。而LGB是基于直方图分裂特征的，梯度信息都存储在一个bin中，所以访问梯度是连续的，缓存命中率高
- 并行策略不同：
  - 特征并行：LGB特征并行的前提是每个worker留有一份完整的数据集，但是每个worker仅在特征子集上进行最佳切分点的寻找；worker之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个worker进行切分即可。XGB的特征并行与LGB的最大不同在于XGB每个worker节点中仅有部分的列数据，也就是垂直切分，每个worker寻找局部最佳切分点，worker之间相互通信，然后在具有最佳切分点的worker上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他worker才能开始分裂。二者的区别就导致了LGB中worker间通信成本明显降低，只需通信一个特征分裂点即可，而XGB中要广播样本索引。
  - 数据并行：当数据量很大，特征相对较少时，可采用数据并行策略。LGB中先对数据水平切分，每个worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得worker间的通信成本降低一倍，因为只用通信以此样本量少的节点。XGB中的数据并行也是水平切分，然后单个worker建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个worker上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个worker间的通信量也就变得很大。
  - 投票并行：当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。大致思想是：每个worker首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择top的特征进行直方图的合并，再寻求全局的最优分割点。

## 8、LR vs 决策树

- 逻辑回归通常用于**分类问题**，决策树可以用于**回归问题和分类问题**
- 逻辑回归可用于**高维稀疏数据场景，比如CTR预估**；决策树变量连续最好，类别变量的话，**稀疏性不能太高**
- **数据整体和局部结构的侧重**
  - 逻辑回归更侧重于数据的整体结构（即全局性），因此其一般不会出现过拟合的情况
  - 对于决策树模型，其更侧重于对数据局部结构的分析（即局部性），因此需要使用修剪的方式来避免过拟合的情况
- 特征空间的划分
  - 逻辑回归更适合处理**线性关系**
  - 而决策树更适合处理**非线性关系**

## 9、GBDT vs LR

从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界根据是否使用核函数可以是一条直线或者曲线，而GBDT的决策边界可能是很多条线。

GBDT并不一定总是好于线性回归或逻辑回归。根据没有免费的午餐原则，没有一个算法是在所有问题上都好于另一个算法的。根据奥卡姆剃刀原则，如果GBDT和线性回归或逻辑回归在某个问题上表现接近，那么我们应该选择相对比较简单的线性回归或逻辑回归。具体选择哪一个算法还是要根据实际问题来决定。
